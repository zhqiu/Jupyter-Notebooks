{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAN 和 Recurrent GAN 的结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GAN:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](./GAN.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 利用GAN生成历史数据"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](./stock_data_gen.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 利用MLP产生一组随机的时间序列（Generator）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions.normal import Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 产生用于输入Generator的噪声, 为正态分布\n",
    "# 其大小为 m x n\n",
    "\n",
    "def get_noise_data(m, n):\n",
    "    dist = Normal(0, 1)\n",
    "    return dist.sample((m, n)).requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_size=20, num_features=4, batch_size=10, seq_len=30):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size   # nums of input rand numbers\n",
    "        self.num_features = num_features\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_len = seq_len\n",
    "        self.output_size = self.num_features * self.seq_len\n",
    "        \n",
    "        # 使用MLP\n",
    "        self.fc1 = nn.Linear(self.input_size, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, self.output_size)\n",
    "        \n",
    "    def forward(self, input_data):\n",
    "        output = torch.sigmoid(self.fc1(input_data))\n",
    "        output = torch.sigmoid(self.fc2(output))\n",
    "        output = torch.sigmoid(self.fc3(output))\n",
    "        \n",
    "        # output size: [batch_size, output_size]\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 20\n",
    "batch_size = 10\n",
    "seq_len = 20\n",
    "\n",
    "g = Generator(input_size=input_size, batch_size=batch_size, seq_len=seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = get_noise_data(batch_size, input_size)\n",
    "\n",
    "time_series = g.forward(input_data)\n",
    "\n",
    "time_series.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 下面是利用MLP构造的Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, batch_size=10, seq_len=30, num_features=4):\n",
    "        super().__init__()\n",
    "        self.input_size = seq_len * num_features\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        # 使用 MLP\n",
    "        self.fc1 = nn.Linear(self.input_size, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.fc4 = nn.Linear(128, 1)\n",
    "        \n",
    "    def forward(self, input_data):\n",
    "        output = torch.sigmoid(self.fc1(input_data))\n",
    "        output = torch.sigmoid(self.fc2(output))\n",
    "        output = torch.sigmoid(self.fc3(output))\n",
    "        output = torch.sigmoid(self.fc4(output))\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一起测试Generator和Discriminator\n",
    "input_size = 20\n",
    "batch_size = 1\n",
    "seq_len = 20\n",
    "\n",
    "g = Generator(input_size=input_size, batch_size=batch_size, seq_len=seq_len)\n",
    "input_data = get_noise_data(batch_size, input_size)\n",
    "time_series = g.forward(input_data)\n",
    "\n",
    "print(time_series.shape)\n",
    "\n",
    "d = Discriminator(batch_size=batch_size, seq_len=seq_len)\n",
    "outputs = d.forward(time_series)\n",
    "\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 获取真实样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从csv文件中读取数据，将数据划分为训练集和测试集\n",
    "# 再从训练集中随机抽取m个长度为n的串返回\n",
    "\n",
    "data = pd.read_csv('stock_data_730.csv')\n",
    "\n",
    "data.set_index([\"date\"], inplace=True)\n",
    "data_sorted = data.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将所有数据分割为训练集和测试集\n",
    "# 与 LSTM -- predict stock price中的函数相同\n",
    "\n",
    "def train_test_split(data, SEQ_LENGTH = 30, test_prop=0.137):  # 0.11 for 1095, 0.137 for 730, 0.3 for 365\n",
    "    \n",
    "    ntrain = int(len(data) *(1-test_prop))  # len(data) = 197\n",
    "    predictors = data.columns[:4]  # open, high, close, low\n",
    "    data_pred = data[predictors]\n",
    "    num_attr = data_pred.shape[1]  # 4\n",
    "    \n",
    "    result = np.empty((len(data) - SEQ_LENGTH, SEQ_LENGTH, num_attr))\n",
    "    y = np.empty((len(data) - SEQ_LENGTH, SEQ_LENGTH))\n",
    "    yopen = np.empty((len(data) - SEQ_LENGTH, SEQ_LENGTH))\n",
    "\n",
    "    for index in range(len(data) - SEQ_LENGTH):\n",
    "        result[index, :, :] = data_pred[index: index + SEQ_LENGTH]\n",
    "        y[index, :] = data_pred[index+1: index + SEQ_LENGTH + 1].close\n",
    "        yopen[index, :] = data_pred[index+1: index + SEQ_LENGTH + 1].open\n",
    "\n",
    "    \"\"\"\n",
    "        xtrain的大小：ntrain x SEQ_LENGTH x 4\n",
    "        ytrain的大小：ntrain x SEQ_LENGTH\n",
    "        \n",
    "        * xtrain的每个batch为长为SEQ_LENGTH的连续序列，一共有ntrain个batch，\n",
    "          序列中每个单元都是一个四元组（open，high，close，low）\n",
    "        * ytrain的每个batch为长为SEQ_LENGTH的连续序列，一共有ntrain个batch，\n",
    "          序列中每个单元是xtrain中对应四元组所在日期的下一天的close price\n",
    "        \n",
    "        xtest 的大小：    ntest x SEQ_LENGTH x 4                \n",
    "        ytest的大小：     ntest x SEQ_LENGTH      (close price)\n",
    "        ytest_open的大小：ntest x SEQ_LENGTH      (open price)  \n",
    "        \n",
    "        * xtest的每个batch为长为SEQ_LENGTH的连续序列，一共有ntest个batch，\n",
    "          序列中每个单元都是一个四元组（open，high，close，low）\n",
    "          每一个序列仅包含一个新四元组，且在最后一个\n",
    "        * ytest的每个batch为长为SEQ_LENGTH的连续序列，一共有ntest个batch，\n",
    "          序列中每个单元是xtest中对应四元组所在日期的下一天的close price\n",
    "        \n",
    "        类型：numpy.ndarray\n",
    "    \"\"\"\n",
    "    xtrain = result[:ntrain, :, :]\n",
    "    ytrain = y[:ntrain]\n",
    "    \n",
    "    xtest = result[ntrain:, :, :]\n",
    "    ytest = y[ntrain:]\n",
    "    ytest_open = yopen[ntrain:]\n",
    "    \n",
    "    return xtrain, xtest, ytrain, ytest, ytest_open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xtest, ytrain, ytest, ytest_open = train_test_split(data_sorted)  # 只需要ytrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取下标从start_idx开始的连续batch_size个序列\n",
    "\n",
    "def get_real_samples(idx, batch_size, data=xtrain):\n",
    "    data = data[idx:idx+batch_size, :]\n",
    "    min_val = np.min(data)\n",
    "    max_val = np.max(data)\n",
    "    mean_val = np.mean(data)\n",
    "    data = (data-mean_val)/(max_val-min_val)\n",
    "    \n",
    "    data = torch.from_numpy(data).float()\n",
    "    data = data.view(batch_size, -1)\n",
    "    \n",
    "    return data.requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_real_samples(0, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将real samples带入Discriminator中进行测试\n",
    "\n",
    "batch_size = 10\n",
    "seq_len = 30\n",
    "\n",
    "real_samples = get_real_samples(0, batch_size)\n",
    "\n",
    "print(real_samples.shape)\n",
    "\n",
    "d = Discriminator(batch_size=batch_size, seq_len=seq_len)\n",
    "outputs = d.forward(real_samples)\n",
    "\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构造GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr=0.1 works !\n",
    "\n",
    "d_learning_rate = 0.001\n",
    "g_learning_rate = 0.001\n",
    "\n",
    "input_size = 20\n",
    "batch_size = 1\n",
    "seq_len = 30\n",
    "\n",
    "G = Generator(input_size=input_size, batch_size=batch_size, seq_len=seq_len)\n",
    "D = Discriminator(batch_size=batch_size, seq_len=seq_len)\n",
    "\n",
    "d_optimizer = optim.SGD(D.parameters(), lr=d_learning_rate) \n",
    "g_optimizer = optim.SGD(G.parameters(), lr=g_learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch     0:  D Loss  55.311 ; G Loss  54.562\n",
      "Epoch     1:  D Loss  10.136 ; G Loss  10.119\n",
      "Epoch     2:  D Loss  5.699 ; G Loss  5.695\n",
      "Epoch     3:  D Loss  3.948 ; G Loss  3.946\n",
      "Epoch     4:  D Loss  3.010 ; G Loss  3.010\n",
      "Epoch     5:  D Loss  2.427 ; G Loss  2.427\n",
      "Epoch     6:  D Loss  2.030 ; G Loss  2.030\n",
      "Epoch     7:  D Loss  1.743 ; G Loss  1.743\n",
      "Epoch     8:  D Loss  1.525 ; G Loss  1.525\n",
      "Epoch     9:  D Loss  1.355 ; G Loss  1.355\n",
      "Epoch    10:  D Loss  1.218 ; G Loss  1.218\n",
      "Epoch    11:  D Loss  1.106 ; G Loss  1.106\n",
      "Epoch    12:  D Loss  1.012 ; G Loss  1.012\n",
      "Epoch    13:  D Loss  0.933 ; G Loss  0.933\n",
      "Epoch    14:  D Loss  0.865 ; G Loss  0.865\n",
      "Epoch    15:  D Loss  0.806 ; G Loss  0.806\n",
      "Epoch    16:  D Loss  0.754 ; G Loss  0.754\n",
      "Epoch    17:  D Loss  0.708 ; G Loss  0.709\n",
      "Epoch    18:  D Loss  0.668 ; G Loss  0.668\n",
      "Epoch    19:  D Loss  0.632 ; G Loss  0.632\n",
      "Epoch    20:  D Loss  0.599 ; G Loss  0.599\n",
      "Epoch    21:  D Loss  0.570 ; G Loss  0.570\n",
      "Epoch    22:  D Loss  0.543 ; G Loss  0.543\n",
      "Epoch    23:  D Loss  0.518 ; G Loss  0.518\n",
      "Epoch    24:  D Loss  0.496 ; G Loss  0.496\n",
      "Epoch    25:  D Loss  0.475 ; G Loss  0.475\n",
      "Epoch    26:  D Loss  0.456 ; G Loss  0.456\n",
      "Epoch    27:  D Loss  0.439 ; G Loss  0.439\n",
      "Epoch    28:  D Loss  0.423 ; G Loss  0.423\n",
      "Epoch    29:  D Loss  0.407 ; G Loss  0.407\n",
      "Epoch    30:  D Loss  0.393 ; G Loss  0.393\n",
      "Epoch    31:  D Loss  0.380 ; G Loss  0.380\n",
      "Epoch    32:  D Loss  0.368 ; G Loss  0.368\n",
      "Epoch    33:  D Loss  0.356 ; G Loss  0.356\n",
      "Epoch    34:  D Loss  0.345 ; G Loss  0.345\n",
      "Epoch    35:  D Loss  0.335 ; G Loss  0.335\n",
      "Epoch    36:  D Loss  0.325 ; G Loss  0.325\n",
      "Epoch    37:  D Loss  0.316 ; G Loss  0.316\n",
      "Epoch    38:  D Loss  0.307 ; G Loss  0.307\n",
      "Epoch    39:  D Loss  0.299 ; G Loss  0.299\n",
      "Epoch    40:  D Loss  0.291 ; G Loss  0.291\n",
      "Epoch    41:  D Loss  0.284 ; G Loss  0.284\n",
      "Epoch    42:  D Loss  0.277 ; G Loss  0.277\n",
      "Epoch    43:  D Loss  0.270 ; G Loss  0.270\n",
      "Epoch    44:  D Loss  0.264 ; G Loss  0.264\n",
      "Epoch    45:  D Loss  0.257 ; G Loss  0.258\n",
      "Epoch    46:  D Loss  0.252 ; G Loss  0.252\n",
      "Epoch    47:  D Loss  0.246 ; G Loss  0.246\n",
      "Epoch    48:  D Loss  0.241 ; G Loss  0.241\n",
      "Epoch    49:  D Loss  0.235 ; G Loss  0.236\n",
      "Epoch    50:  D Loss  0.231 ; G Loss  0.231\n",
      "Epoch    51:  D Loss  0.226 ; G Loss  0.226\n",
      "Epoch    52:  D Loss  0.221 ; G Loss  0.221\n",
      "Epoch    53:  D Loss  0.217 ; G Loss  0.217\n",
      "Epoch    54:  D Loss  0.213 ; G Loss  0.213\n",
      "Epoch    55:  D Loss  0.209 ; G Loss  0.209\n",
      "Epoch    56:  D Loss  0.205 ; G Loss  0.205\n",
      "Epoch    57:  D Loss  0.201 ; G Loss  0.201\n",
      "Epoch    58:  D Loss  0.197 ; G Loss  0.197\n",
      "Epoch    59:  D Loss  0.194 ; G Loss  0.194\n",
      "Epoch    60:  D Loss  0.190 ; G Loss  0.191\n",
      "Epoch    61:  D Loss  0.187 ; G Loss  0.187\n",
      "Epoch    62:  D Loss  0.184 ; G Loss  0.184\n",
      "Epoch    63:  D Loss  0.181 ; G Loss  0.181\n",
      "Epoch    64:  D Loss  0.178 ; G Loss  0.178\n",
      "Epoch    65:  D Loss  0.175 ; G Loss  0.175\n",
      "Epoch    66:  D Loss  0.172 ; G Loss  0.172\n",
      "Epoch    67:  D Loss  0.170 ; G Loss  0.170\n",
      "Epoch    68:  D Loss  0.167 ; G Loss  0.167\n",
      "Epoch    69:  D Loss  0.164 ; G Loss  0.165\n",
      "Epoch    70:  D Loss  0.162 ; G Loss  0.162\n",
      "Epoch    71:  D Loss  0.160 ; G Loss  0.160\n",
      "Epoch    72:  D Loss  0.157 ; G Loss  0.157\n",
      "Epoch    73:  D Loss  0.155 ; G Loss  0.155\n",
      "Epoch    74:  D Loss  0.153 ; G Loss  0.153\n",
      "Epoch    75:  D Loss  0.151 ; G Loss  0.151\n",
      "Epoch    76:  D Loss  0.149 ; G Loss  0.149\n",
      "Epoch    77:  D Loss  0.147 ; G Loss  0.147\n",
      "Epoch    78:  D Loss  0.145 ; G Loss  0.145\n",
      "Epoch    79:  D Loss  0.143 ; G Loss  0.143\n",
      "Epoch    80:  D Loss  0.141 ; G Loss  0.141\n",
      "Epoch    81:  D Loss  0.139 ; G Loss  0.139\n",
      "Epoch    82:  D Loss  0.137 ; G Loss  0.137\n",
      "Epoch    83:  D Loss  0.135 ; G Loss  0.135\n",
      "Epoch    84:  D Loss  0.134 ; G Loss  0.134\n",
      "Epoch    85:  D Loss  0.132 ; G Loss  0.132\n",
      "Epoch    86:  D Loss  0.130 ; G Loss  0.131\n",
      "Epoch    87:  D Loss  0.129 ; G Loss  0.129\n",
      "Epoch    88:  D Loss  0.127 ; G Loss  0.127\n",
      "Epoch    89:  D Loss  0.126 ; G Loss  0.126\n",
      "Epoch    90:  D Loss  0.124 ; G Loss  0.124\n",
      "Epoch    91:  D Loss  0.123 ; G Loss  0.123\n",
      "Epoch    92:  D Loss  0.122 ; G Loss  0.122\n",
      "Epoch    93:  D Loss  0.120 ; G Loss  0.120\n",
      "Epoch    94:  D Loss  0.119 ; G Loss  0.119\n",
      "Epoch    95:  D Loss  0.117 ; G Loss  0.118\n",
      "Epoch    96:  D Loss  0.116 ; G Loss  0.116\n",
      "Epoch    97:  D Loss  0.115 ; G Loss  0.115\n",
      "Epoch    98:  D Loss  0.114 ; G Loss  0.114\n",
      "Epoch    99:  D Loss  0.112 ; G Loss  0.113\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    G.train()\n",
    "    D.train()\n",
    "    \n",
    "    sum_real_error = 0\n",
    "    sum_gen_loss = 0\n",
    "    \n",
    "    for idx in range(0, len(xtrain)-batch_size+1, batch_size):\n",
    "        \n",
    "        \"\"\"\n",
    "            训练Discriminator\n",
    "        \"\"\"\n",
    "        D.zero_grad()\n",
    "        # 用真实样本训练D \n",
    "        real_data = get_real_samples(idx, batch_size)\n",
    "        real_decision = D(real_data)\n",
    "        #print(real_decision)\n",
    "        real_error = -torch.sum(torch.log(real_decision))/batch_size\n",
    "        #print(real_error)\n",
    "        sum_real_error += real_error\n",
    "        real_error.backward()\n",
    "    \n",
    "        # 用生成样本训练D\n",
    "        input_data = get_noise_data(batch_size, input_size)\n",
    "        fake_data = G.forward(input_data) \n",
    "        fake_decision = D(fake_data)\n",
    "        #print(fake_decision)\n",
    "        fake_error = 1 - torch.sum(torch.log(fake_decision))/batch_size\n",
    "        #print(fake_error)\n",
    "        fake_error.backward()\n",
    "    \n",
    "        d_optimizer.step()\n",
    "    \n",
    "    \n",
    "        \"\"\"\n",
    "            训练Generator\n",
    "        \"\"\"\n",
    "        G.zero_grad()\n",
    "        # 训练G\n",
    "        input_data = get_noise_data(batch_size, input_size)\n",
    "        fake_data = G.forward(input_data)\n",
    "        fake_decision = D(fake_data)\n",
    "        #print(fake_decision)\n",
    "        gen_loss = -torch.sum(torch.log(fake_decision))/batch_size\n",
    "        #print(gen_loss)\n",
    "        sum_gen_loss += gen_loss\n",
    "        gen_loss.backward()\n",
    "\n",
    "        g_optimizer.step()\n",
    "    \n",
    "    print(\"Epoch %5d:  D Loss  %5.3f ; \" % (epoch, sum_real_error), end=\"\")\n",
    "    print(\"G Loss  %5.3f\" % (sum_gen_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
