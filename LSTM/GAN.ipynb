{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAN 和 Recurrent GAN 的结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GAN:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](./GAN.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 利用GAN生成历史数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](./stock_data_gen.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 利用MLP产生一组随机的时间序列（Generator）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions.normal import Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 产生用于输入Generator的噪声, 为正态分布\n",
    "# 其大小为 m x n\n",
    "\n",
    "def get_noise_data(m, n):\n",
    "    dist = Normal(0, 1)\n",
    "    return dist.sample((m, n)).requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_size=20, num_features=4, batch_size=10, seq_len=26):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size   # nums of input rand numbers\n",
    "        self.num_features = num_features\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_len = seq_len\n",
    "        self.output_size = self.num_features * self.seq_len\n",
    "        \n",
    "        # 使用MLP\n",
    "        self.fc1 = nn.Linear(self.input_size, 256)\n",
    "        self.fc2 = nn.Linear(256, 512)\n",
    "        self.fc3 = nn.Linear(512, self.output_size)\n",
    "        \n",
    "    def forward(self, input_data):\n",
    "        output = torch.sigmoid(self.fc1(input_data))\n",
    "        output = torch.sigmoid(self.fc2(output))\n",
    "        output = torch.sigmoid(self.fc3(output))\n",
    "        \n",
    "        # output size: [batch_size, output_size]\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 20\n",
    "batch_size = 10\n",
    "seq_len = 26\n",
    "\n",
    "g = Generator(input_size=input_size, batch_size=batch_size, seq_len=seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 104])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data = get_noise_data(batch_size, input_size)\n",
    "\n",
    "time_series = g.forward(input_data)\n",
    "\n",
    "time_series.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 下面是利用MLP构造的Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, batch_size=10, seq_len=26, num_features=4):\n",
    "        super().__init__()\n",
    "        self.input_size = seq_len * num_features\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        # 使用 MLP\n",
    "        self.fc1 = nn.Linear(self.input_size, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.fc4 = nn.Linear(128, 1)\n",
    "        \n",
    "    def forward(self, input_data):\n",
    "        output = torch.sigmoid(self.fc1(input_data))\n",
    "        output = torch.sigmoid(self.fc2(output))\n",
    "        output = torch.sigmoid(self.fc3(output))\n",
    "        output = torch.sigmoid(self.fc4(output))\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 104])\n",
      "tensor([[0.4655]], grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "# 一起测试Generator和Discriminator\n",
    "input_size = 20\n",
    "batch_size = 1\n",
    "seq_len = 26\n",
    "\n",
    "g = Generator(input_size=input_size, batch_size=batch_size, seq_len=seq_len)\n",
    "input_data = get_noise_data(batch_size, input_size)\n",
    "time_series = g.forward(input_data)\n",
    "\n",
    "print(time_series.shape)\n",
    "\n",
    "d = Discriminator(batch_size=batch_size, seq_len=seq_len)\n",
    "outputs = d.forward(time_series)\n",
    "\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 获取真实样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从csv文件中读取数据，将数据划分为训练集和测试集\n",
    "# 再从训练集中随机抽取m个长度为n的串返回\n",
    "\n",
    "data = pd.read_csv('stock_data_730.csv')\n",
    "\n",
    "data.set_index([\"date\"], inplace=True)\n",
    "data_sorted = data.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将所有数据分割为训练集和测试集\n",
    "# 与 LSTM -- predict stock price中的函数相同\n",
    "\n",
    "def train_test_split(data, SEQ_LENGTH = 26, test_prop=0.137):  # 0.11 for 1095, 0.137 for 730, 0.3 for 365\n",
    "    \n",
    "    ntrain = int(len(data) *(1-test_prop))  # len(data) = 197\n",
    "    predictors = data.columns[:4]  # open, high, close, low\n",
    "    data_pred = data[predictors]\n",
    "    num_attr = data_pred.shape[1]  # 4\n",
    "    \n",
    "    result = np.empty((len(data) - SEQ_LENGTH, SEQ_LENGTH, num_attr))\n",
    "    y = np.empty((len(data) - SEQ_LENGTH, SEQ_LENGTH))\n",
    "    yopen = np.empty((len(data) - SEQ_LENGTH, SEQ_LENGTH))\n",
    "\n",
    "    for index in range(len(data) - SEQ_LENGTH):\n",
    "        result[index, :, :] = data_pred[index: index + SEQ_LENGTH]\n",
    "        y[index, :] = data_pred[index+1: index + SEQ_LENGTH + 1].close\n",
    "        yopen[index, :] = data_pred[index+1: index + SEQ_LENGTH + 1].open\n",
    "\n",
    "    \"\"\"\n",
    "        xtrain的大小：ntrain x SEQ_LENGTH x 4\n",
    "        ytrain的大小：ntrain x SEQ_LENGTH\n",
    "        \n",
    "        * xtrain的每个batch为长为SEQ_LENGTH的连续序列，一共有ntrain个batch，\n",
    "          序列中每个单元都是一个四元组（open，high，close，low）\n",
    "        * ytrain的每个batch为长为SEQ_LENGTH的连续序列，一共有ntrain个batch，\n",
    "          序列中每个单元是xtrain中对应四元组所在日期的下一天的close price\n",
    "        \n",
    "        xtest 的大小：    ntest x SEQ_LENGTH x 4                \n",
    "        ytest的大小：     ntest x SEQ_LENGTH      (close price)\n",
    "        ytest_open的大小：ntest x SEQ_LENGTH      (open price)  \n",
    "        \n",
    "        * xtest的每个batch为长为SEQ_LENGTH的连续序列，一共有ntest个batch，\n",
    "          序列中每个单元都是一个四元组（open，high，close，low）\n",
    "          每一个序列仅包含一个新四元组，且在最后一个\n",
    "        * ytest的每个batch为长为SEQ_LENGTH的连续序列，一共有ntest个batch，\n",
    "          序列中每个单元是xtest中对应四元组所在日期的下一天的close price\n",
    "        \n",
    "        类型：numpy.ndarray\n",
    "    \"\"\"\n",
    "    xtrain = result[:ntrain, :, :]\n",
    "    ytrain = y[:ntrain]\n",
    "    \n",
    "    xtest = result[ntrain:, :, :]\n",
    "    ytest = y[ntrain:]\n",
    "    ytest_open = yopen[ntrain:]\n",
    "    \n",
    "    return xtrain, xtest, ytrain, ytest, ytest_open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xtest, ytrain, ytest, ytest_open = train_test_split(data_sorted)  # 只需要xtrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.418522797766748 21.26 9.86\n"
     ]
    }
   ],
   "source": [
    "xtrain.shape  # open, high, close, low\n",
    "\n",
    "xtrain_mean = np.mean(xtrain)\n",
    "xtrain_max = np.max(xtrain)\n",
    "xtrain_min = np.min(xtrain)\n",
    "\n",
    "print(xtrain_mean, xtrain_max, xtrain_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取下标从start_idx开始的连续batch_size个序列\n",
    "\n",
    "def get_real_samples(idx, batch_size, data=xtrain):\n",
    "    data = data[idx:idx+batch_size, :]\n",
    "    min_val = np.min(data)\n",
    "    max_val = np.max(data)\n",
    "    mean_val = np.mean(data)\n",
    "    data = (data-mean_val)/(max_val-min_val)\n",
    "    \n",
    "    data = torch.from_numpy(data).float()\n",
    "    data = data.view(batch_size, -1)\n",
    "    \n",
    "    return data.requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 104])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_real_samples(0, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 104])\n",
      "tensor([[0.5846],\n",
      "        [0.5846],\n",
      "        [0.5845],\n",
      "        [0.5845],\n",
      "        [0.5845],\n",
      "        [0.5845],\n",
      "        [0.5844],\n",
      "        [0.5844],\n",
      "        [0.5844],\n",
      "        [0.5844]], grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "# 将real samples带入Discriminator中进行测试\n",
    "\n",
    "batch_size = 10\n",
    "seq_len = 26\n",
    "\n",
    "real_samples = get_real_samples(0, batch_size)\n",
    "\n",
    "print(real_samples.shape)\n",
    "\n",
    "d = Discriminator(batch_size=batch_size, seq_len=seq_len)\n",
    "outputs = d.forward(real_samples)\n",
    "\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构造GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_learning_rate = 0.01\n",
    "g_learning_rate = 0.01\n",
    "\n",
    "input_size = 20\n",
    "batch_size = 5\n",
    "seq_len = 26\n",
    "\n",
    "G = Generator(input_size=input_size, batch_size=batch_size, seq_len=seq_len)\n",
    "D = Discriminator(batch_size=batch_size, seq_len=seq_len)\n",
    "\n",
    "d_optimizer = optim.SGD(D.parameters(), lr=d_learning_rate) \n",
    "g_optimizer = optim.SGD(G.parameters(), lr=g_learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch     0:  D Loss  6.216  G Loss  5.506  duration: 0.209405\n",
      "Epoch     1:  D Loss  0.933  G Loss  0.922  duration: 0.204474\n",
      "Epoch     2:  D Loss  0.529  G Loss  0.525  duration: 0.230384\n",
      "Epoch     3:  D Loss  0.369  G Loss  0.366  duration: 0.201478\n",
      "Epoch     4:  D Loss  0.282  G Loss  0.280  duration: 0.203907\n",
      "Epoch     5:  D Loss  0.228  G Loss  0.227  duration: 0.195495\n",
      "Epoch     6:  D Loss  0.191  G Loss  0.190  duration: 0.204188\n",
      "Epoch     7:  D Loss  0.164  G Loss  0.163  duration: 0.201815\n",
      "Epoch     8:  D Loss  0.144  G Loss  0.143  duration: 0.203459\n",
      "Epoch     9:  D Loss  0.128  G Loss  0.127  duration: 0.211434\n",
      "Epoch    10:  D Loss  0.115  G Loss  0.114  duration: 0.197472\n",
      "Epoch    11:  D Loss  0.104  G Loss  0.104  duration: 0.205986\n",
      "Epoch    12:  D Loss  0.096  G Loss  0.095  duration: 0.201302\n",
      "Epoch    13:  D Loss  0.088  G Loss  0.088  duration: 0.202085\n",
      "Epoch    14:  D Loss  0.082  G Loss  0.081  duration: 0.205161\n",
      "Epoch    15:  D Loss  0.076  G Loss  0.076  duration: 0.208534\n",
      "Epoch    16:  D Loss  0.071  G Loss  0.071  duration: 0.250320\n",
      "Epoch    17:  D Loss  0.067  G Loss  0.067  duration: 0.212940\n",
      "Epoch    18:  D Loss  0.063  G Loss  0.063  duration: 0.199466\n",
      "Epoch    19:  D Loss  0.060  G Loss  0.059  duration: 0.207445\n",
      "Epoch    20:  D Loss  0.057  G Loss  0.056  duration: 0.197472\n",
      "Epoch    21:  D Loss  0.054  G Loss  0.054  duration: 0.206448\n",
      "Epoch    22:  D Loss  0.051  G Loss  0.051  duration: 0.198469\n",
      "Epoch    23:  D Loss  0.049  G Loss  0.049  duration: 0.207664\n",
      "Epoch    24:  D Loss  0.047  G Loss  0.047  duration: 0.198560\n",
      "Epoch    25:  D Loss  0.045  G Loss  0.045  duration: 0.204465\n",
      "Epoch    26:  D Loss  0.043  G Loss  0.043  duration: 0.203456\n",
      "Epoch    27:  D Loss  0.042  G Loss  0.041  duration: 0.221412\n",
      "Epoch    28:  D Loss  0.040  G Loss  0.040  duration: 0.265192\n",
      "Epoch    29:  D Loss  0.039  G Loss  0.038  duration: 0.211900\n",
      "Epoch    30:  D Loss  0.037  G Loss  0.037  duration: 0.214015\n",
      "Epoch    31:  D Loss  0.036  G Loss  0.036  duration: 0.202845\n",
      "Epoch    32:  D Loss  0.035  G Loss  0.035  duration: 0.207291\n",
      "Epoch    33:  D Loss  0.034  G Loss  0.034  duration: 0.208443\n",
      "Epoch    34:  D Loss  0.033  G Loss  0.033  duration: 0.210475\n",
      "Epoch    35:  D Loss  0.032  G Loss  0.032  duration: 0.212434\n",
      "Epoch    36:  D Loss  0.031  G Loss  0.031  duration: 0.209342\n",
      "Epoch    37:  D Loss  0.030  G Loss  0.030  duration: 0.207608\n",
      "Epoch    38:  D Loss  0.029  G Loss  0.029  duration: 0.212432\n",
      "Epoch    39:  D Loss  0.028  G Loss  0.028  duration: 0.212697\n",
      "Epoch    40:  D Loss  0.028  G Loss  0.027  duration: 0.209440\n",
      "Epoch    41:  D Loss  0.027  G Loss  0.027  duration: 0.209714\n",
      "Epoch    42:  D Loss  0.026  G Loss  0.026  duration: 0.207813\n",
      "Epoch    43:  D Loss  0.026  G Loss  0.025  duration: 0.204621\n",
      "Epoch    44:  D Loss  0.025  G Loss  0.025  duration: 0.217575\n",
      "Epoch    45:  D Loss  0.024  G Loss  0.024  duration: 0.216421\n",
      "Epoch    46:  D Loss  0.024  G Loss  0.024  duration: 0.208442\n",
      "Epoch    47:  D Loss  0.023  G Loss  0.023  duration: 0.212432\n",
      "Epoch    48:  D Loss  0.023  G Loss  0.023  duration: 0.214433\n",
      "Epoch    49:  D Loss  0.022  G Loss  0.022  duration: 0.208442\n",
      "Epoch    50:  D Loss  0.022  G Loss  0.022  duration: 0.210437\n",
      "Epoch    51:  D Loss  0.021  G Loss  0.021  duration: 0.211435\n",
      "Epoch    52:  D Loss  0.021  G Loss  0.021  duration: 0.209494\n",
      "Epoch    53:  D Loss  0.021  G Loss  0.020  duration: 0.217419\n",
      "Epoch    54:  D Loss  0.020  G Loss  0.020  duration: 0.219413\n",
      "Epoch    55:  D Loss  0.020  G Loss  0.020  duration: 0.213434\n",
      "Epoch    56:  D Loss  0.019  G Loss  0.019  duration: 0.213127\n",
      "Epoch    57:  D Loss  0.019  G Loss  0.019  duration: 0.207474\n",
      "Epoch    58:  D Loss  0.019  G Loss  0.019  duration: 0.213578\n",
      "Epoch    59:  D Loss  0.018  G Loss  0.018  duration: 0.211436\n",
      "Epoch    60:  D Loss  0.018  G Loss  0.018  duration: 0.209625\n",
      "Epoch    61:  D Loss  0.018  G Loss  0.018  duration: 0.217633\n",
      "Epoch    62:  D Loss  0.017  G Loss  0.017  duration: 0.251328\n",
      "Epoch    63:  D Loss  0.017  G Loss  0.017  duration: 0.273499\n",
      "Epoch    64:  D Loss  0.017  G Loss  0.017  duration: 0.291891\n",
      "Epoch    65:  D Loss  0.017  G Loss  0.017  duration: 0.289669\n",
      "Epoch    66:  D Loss  0.016  G Loss  0.016  duration: 0.297206\n",
      "Epoch    67:  D Loss  0.016  G Loss  0.016  duration: 0.293157\n",
      "Epoch    68:  D Loss  0.016  G Loss  0.016  duration: 0.290246\n",
      "Epoch    69:  D Loss  0.016  G Loss  0.016  duration: 0.293671\n",
      "Epoch    70:  D Loss  0.015  G Loss  0.015  duration: 0.299884\n",
      "Epoch    71:  D Loss  0.015  G Loss  0.015  duration: 0.292313\n",
      "Epoch    72:  D Loss  0.015  G Loss  0.015  duration: 0.294154\n",
      "Epoch    73:  D Loss  0.015  G Loss  0.015  duration: 0.290543\n",
      "Epoch    74:  D Loss  0.014  G Loss  0.014  duration: 0.295387\n",
      "Epoch    75:  D Loss  0.014  G Loss  0.014  duration: 0.288812\n",
      "Epoch    76:  D Loss  0.014  G Loss  0.014  duration: 0.296363\n",
      "Epoch    77:  D Loss  0.014  G Loss  0.014  duration: 0.286728\n",
      "Epoch    78:  D Loss  0.014  G Loss  0.014  duration: 0.294980\n",
      "Epoch    79:  D Loss  0.014  G Loss  0.013  duration: 0.281358\n",
      "Epoch    80:  D Loss  0.013  G Loss  0.013  duration: 0.295210\n",
      "Epoch    81:  D Loss  0.013  G Loss  0.013  duration: 0.286240\n",
      "Epoch    82:  D Loss  0.013  G Loss  0.013  duration: 0.293220\n",
      "Epoch    83:  D Loss  0.013  G Loss  0.013  duration: 0.293216\n",
      "Epoch    84:  D Loss  0.013  G Loss  0.013  duration: 0.286234\n",
      "Epoch    85:  D Loss  0.013  G Loss  0.012  duration: 0.292219\n",
      "Epoch    86:  D Loss  0.012  G Loss  0.012  duration: 0.287763\n",
      "Epoch    87:  D Loss  0.012  G Loss  0.012  duration: 0.294303\n",
      "Epoch    88:  D Loss  0.012  G Loss  0.012  duration: 0.291511\n",
      "Epoch    89:  D Loss  0.012  G Loss  0.012  duration: 0.288837\n",
      "Epoch    90:  D Loss  0.012  G Loss  0.012  duration: 0.292081\n",
      "Epoch    91:  D Loss  0.012  G Loss  0.012  duration: 0.291886\n",
      "Epoch    92:  D Loss  0.012  G Loss  0.011  duration: 0.288234\n",
      "Epoch    93:  D Loss  0.011  G Loss  0.011  duration: 0.292220\n",
      "Epoch    94:  D Loss  0.011  G Loss  0.011  duration: 0.285237\n",
      "Epoch    95:  D Loss  0.011  G Loss  0.011  duration: 0.291260\n",
      "Epoch    96:  D Loss  0.011  G Loss  0.011  duration: 0.287455\n",
      "Epoch    97:  D Loss  0.011  G Loss  0.011  duration: 0.290605\n",
      "Epoch    98:  D Loss  0.011  G Loss  0.011  duration: 0.298504\n",
      "Epoch    99:  D Loss  0.011  G Loss  0.011  duration: 0.281521\n",
      "Epoch   100:  D Loss  0.011  G Loss  0.010  duration: 0.289289\n",
      "Epoch   101:  D Loss  0.010  G Loss  0.010  duration: 0.294217\n",
      "Epoch   102:  D Loss  0.010  G Loss  0.010  duration: 0.288230\n",
      "Epoch   103:  D Loss  0.010  G Loss  0.010  duration: 0.289849\n",
      "Epoch   104:  D Loss  0.010  G Loss  0.010  duration: 0.286453\n",
      "Epoch   105:  D Loss  0.010  G Loss  0.010  duration: 0.290760\n",
      "Epoch   106:  D Loss  0.010  G Loss  0.010  duration: 0.290310\n",
      "Epoch   107:  D Loss  0.010  G Loss  0.010  duration: 0.302192\n",
      "Epoch   108:  D Loss  0.010  G Loss  0.010  duration: 0.291336\n",
      "Epoch   109:  D Loss  0.010  G Loss  0.010  duration: 0.290098\n",
      "Epoch   110:  D Loss  0.010  G Loss  0.009  duration: 0.293221\n",
      "Epoch   111:  D Loss  0.009  G Loss  0.009  duration: 0.291981\n",
      "Epoch   112:  D Loss  0.009  G Loss  0.009  duration: 0.287671\n",
      "Epoch   113:  D Loss  0.009  G Loss  0.009  duration: 0.287235\n",
      "Epoch   114:  D Loss  0.009  G Loss  0.009  duration: 0.290535\n",
      "Epoch   115:  D Loss  0.009  G Loss  0.009  duration: 0.289613\n",
      "Epoch   116:  D Loss  0.009  G Loss  0.009  duration: 0.315157\n",
      "Epoch   117:  D Loss  0.009  G Loss  0.009  duration: 0.294231\n",
      "Epoch   118:  D Loss  0.009  G Loss  0.009  duration: 0.288321\n",
      "Epoch   119:  D Loss  0.009  G Loss  0.009  duration: 0.288229\n",
      "Epoch   120:  D Loss  0.009  G Loss  0.009  duration: 0.288057\n",
      "Epoch   121:  D Loss  0.009  G Loss  0.009  duration: 0.288087\n",
      "Epoch   122:  D Loss  0.009  G Loss  0.009  duration: 0.291231\n",
      "Epoch   123:  D Loss  0.008  G Loss  0.008  duration: 0.293756\n",
      "Epoch   124:  D Loss  0.008  G Loss  0.008  duration: 0.288242\n",
      "Epoch   125:  D Loss  0.008  G Loss  0.008  duration: 0.296330\n",
      "Epoch   126:  D Loss  0.008  G Loss  0.008  duration: 0.294639\n",
      "Epoch   127:  D Loss  0.008  G Loss  0.008  duration: 0.285957\n",
      "Epoch   128:  D Loss  0.008  G Loss  0.008  duration: 0.291350\n",
      "Epoch   129:  D Loss  0.008  G Loss  0.008  duration: 0.291226\n",
      "Epoch   130:  D Loss  0.008  G Loss  0.008  duration: 0.293444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   131:  D Loss  0.008  G Loss  0.008  duration: 0.285774\n",
      "Epoch   132:  D Loss  0.008  G Loss  0.008  duration: 0.289350\n",
      "Epoch   133:  D Loss  0.008  G Loss  0.008  duration: 0.288238\n",
      "Epoch   134:  D Loss  0.008  G Loss  0.008  duration: 0.291479\n",
      "Epoch   135:  D Loss  0.008  G Loss  0.008  duration: 0.288331\n",
      "Epoch   136:  D Loss  0.008  G Loss  0.008  duration: 0.283194\n",
      "Epoch   137:  D Loss  0.008  G Loss  0.008  duration: 0.287470\n",
      "Epoch   138:  D Loss  0.008  G Loss  0.007  duration: 0.291225\n",
      "Epoch   139:  D Loss  0.007  G Loss  0.007  duration: 0.288110\n",
      "Epoch   140:  D Loss  0.007  G Loss  0.007  duration: 0.286235\n",
      "Epoch   141:  D Loss  0.007  G Loss  0.007  duration: 0.285439\n",
      "Epoch   142:  D Loss  0.007  G Loss  0.007  duration: 0.291230\n",
      "Epoch   143:  D Loss  0.007  G Loss  0.007  duration: 0.284239\n",
      "Epoch   144:  D Loss  0.007  G Loss  0.007  duration: 0.286484\n",
      "Epoch   145:  D Loss  0.007  G Loss  0.007  duration: 0.289044\n",
      "Epoch   146:  D Loss  0.007  G Loss  0.007  duration: 0.297205\n",
      "Epoch   147:  D Loss  0.007  G Loss  0.007  duration: 0.285843\n",
      "Epoch   148:  D Loss  0.007  G Loss  0.007  duration: 0.288522\n",
      "Epoch   149:  D Loss  0.007  G Loss  0.007  duration: 0.286240\n",
      "Epoch   150:  D Loss  0.007  G Loss  0.007  duration: 0.288229\n",
      "Epoch   151:  D Loss  0.007  G Loss  0.007  duration: 0.286235\n",
      "Epoch   152:  D Loss  0.007  G Loss  0.007  duration: 0.290225\n",
      "Epoch   153:  D Loss  0.007  G Loss  0.007  duration: 0.286238\n",
      "Epoch   154:  D Loss  0.007  G Loss  0.007  duration: 0.286242\n",
      "Epoch   155:  D Loss  0.007  G Loss  0.007  duration: 0.287231\n",
      "Epoch   156:  D Loss  0.007  G Loss  0.007  duration: 0.284096\n",
      "Epoch   157:  D Loss  0.007  G Loss  0.007  duration: 0.290236\n",
      "Epoch   158:  D Loss  0.007  G Loss  0.006  duration: 0.290119\n",
      "Epoch   159:  D Loss  0.006  G Loss  0.006  duration: 0.289226\n",
      "Epoch   160:  D Loss  0.006  G Loss  0.006  duration: 0.289227\n",
      "Epoch   161:  D Loss  0.006  G Loss  0.006  duration: 0.288669\n",
      "Epoch   162:  D Loss  0.006  G Loss  0.006  duration: 0.288228\n",
      "Epoch   163:  D Loss  0.006  G Loss  0.006  duration: 0.291734\n",
      "Epoch   164:  D Loss  0.006  G Loss  0.006  duration: 0.288229\n",
      "Epoch   165:  D Loss  0.006  G Loss  0.006  duration: 0.288229\n",
      "Epoch   166:  D Loss  0.006  G Loss  0.006  duration: 0.290729\n",
      "Epoch   167:  D Loss  0.006  G Loss  0.006  duration: 0.284240\n",
      "Epoch   168:  D Loss  0.006  G Loss  0.006  duration: 0.284898\n",
      "Epoch   169:  D Loss  0.006  G Loss  0.006  duration: 0.292244\n",
      "Epoch   170:  D Loss  0.006  G Loss  0.006  duration: 0.286239\n",
      "Epoch   171:  D Loss  0.006  G Loss  0.006  duration: 0.287231\n",
      "Epoch   172:  D Loss  0.006  G Loss  0.006  duration: 0.287350\n",
      "Epoch   173:  D Loss  0.006  G Loss  0.006  duration: 0.283515\n",
      "Epoch   174:  D Loss  0.006  G Loss  0.006  duration: 0.292219\n",
      "Epoch   175:  D Loss  0.006  G Loss  0.006  duration: 0.293256\n",
      "Epoch   176:  D Loss  0.006  G Loss  0.006  duration: 0.289204\n",
      "Epoch   177:  D Loss  0.006  G Loss  0.006  duration: 0.286241\n",
      "Epoch   178:  D Loss  0.006  G Loss  0.006  duration: 0.286582\n",
      "Epoch   179:  D Loss  0.006  G Loss  0.006  duration: 0.303813\n",
      "Epoch   180:  D Loss  0.006  G Loss  0.006  duration: 0.292218\n",
      "Epoch   181:  D Loss  0.006  G Loss  0.006  duration: 0.285237\n",
      "Epoch   182:  D Loss  0.006  G Loss  0.006  duration: 0.286238\n",
      "Epoch   183:  D Loss  0.006  G Loss  0.006  duration: 0.287592\n",
      "Epoch   184:  D Loss  0.006  G Loss  0.006  duration: 0.287389\n",
      "Epoch   185:  D Loss  0.006  G Loss  0.005  duration: 0.291215\n",
      "Epoch   186:  D Loss  0.005  G Loss  0.005  duration: 0.290254\n",
      "Epoch   187:  D Loss  0.005  G Loss  0.005  duration: 0.284196\n",
      "Epoch   188:  D Loss  0.005  G Loss  0.005  duration: 0.286234\n",
      "Epoch   189:  D Loss  0.005  G Loss  0.005  duration: 0.288229\n",
      "Epoch   190:  D Loss  0.005  G Loss  0.005  duration: 0.288234\n",
      "Epoch   191:  D Loss  0.005  G Loss  0.005  duration: 0.288230\n",
      "Epoch   192:  D Loss  0.005  G Loss  0.005  duration: 0.286478\n",
      "Epoch   193:  D Loss  0.005  G Loss  0.005  duration: 0.281248\n",
      "Epoch   194:  D Loss  0.005  G Loss  0.005  duration: 0.297485\n",
      "Epoch   195:  D Loss  0.005  G Loss  0.005  duration: 0.283892\n",
      "Epoch   196:  D Loss  0.005  G Loss  0.005  duration: 0.287732\n",
      "Epoch   197:  D Loss  0.005  G Loss  0.005  duration: 0.287233\n",
      "Epoch   198:  D Loss  0.005  G Loss  0.005  duration: 0.292219\n",
      "Epoch   199:  D Loss  0.005  G Loss  0.005  duration: 0.288086\n",
      "Epoch   200:  D Loss  0.005  G Loss  0.005  duration: 0.290429\n",
      "Epoch   201:  D Loss  0.005  G Loss  0.005  duration: 0.291798\n",
      "Epoch   202:  D Loss  0.005  G Loss  0.005  duration: 0.285246\n",
      "Epoch   203:  D Loss  0.005  G Loss  0.005  duration: 0.292218\n",
      "Epoch   204:  D Loss  0.005  G Loss  0.005  duration: 0.287000\n",
      "Epoch   205:  D Loss  0.005  G Loss  0.005  duration: 0.294217\n",
      "Epoch   206:  D Loss  0.005  G Loss  0.005  duration: 0.283794\n",
      "Epoch   207:  D Loss  0.005  G Loss  0.005  duration: 0.288229\n",
      "Epoch   208:  D Loss  0.005  G Loss  0.005  duration: 0.288229\n",
      "Epoch   209:  D Loss  0.005  G Loss  0.005  duration: 0.287180\n",
      "Epoch   210:  D Loss  0.005  G Loss  0.005  duration: 0.287225\n",
      "Epoch   211:  D Loss  0.005  G Loss  0.005  duration: 0.288660\n",
      "Epoch   212:  D Loss  0.005  G Loss  0.005  duration: 0.287240\n",
      "Epoch   213:  D Loss  0.005  G Loss  0.005  duration: 0.287236\n",
      "Epoch   214:  D Loss  0.005  G Loss  0.005  duration: 0.285237\n",
      "Epoch   215:  D Loss  0.005  G Loss  0.005  duration: 0.290231\n",
      "Epoch   216:  D Loss  0.005  G Loss  0.005  duration: 0.288229\n",
      "Epoch   217:  D Loss  0.005  G Loss  0.005  duration: 0.291226\n",
      "Epoch   218:  D Loss  0.005  G Loss  0.005  duration: 0.287045\n",
      "Epoch   219:  D Loss  0.005  G Loss  0.005  duration: 0.285167\n",
      "Epoch   220:  D Loss  0.005  G Loss  0.005  duration: 0.294605\n",
      "Epoch   221:  D Loss  0.005  G Loss  0.005  duration: 0.290224\n",
      "Epoch   222:  D Loss  0.005  G Loss  0.005  duration: 0.290402\n",
      "Epoch   223:  D Loss  0.005  G Loss  0.005  duration: 0.289227\n",
      "Epoch   224:  D Loss  0.005  G Loss  0.004  duration: 0.286131\n",
      "Epoch   225:  D Loss  0.004  G Loss  0.004  duration: 0.289226\n",
      "Epoch   226:  D Loss  0.004  G Loss  0.004  duration: 0.289228\n",
      "Epoch   227:  D Loss  0.004  G Loss  0.004  duration: 0.288234\n",
      "Epoch   228:  D Loss  0.004  G Loss  0.004  duration: 0.297205\n",
      "Epoch   229:  D Loss  0.004  G Loss  0.004  duration: 0.292116\n",
      "Epoch   230:  D Loss  0.004  G Loss  0.004  duration: 0.285243\n",
      "Epoch   231:  D Loss  0.004  G Loss  0.004  duration: 0.288099\n",
      "Epoch   232:  D Loss  0.004  G Loss  0.004  duration: 0.292227\n",
      "Epoch   233:  D Loss  0.004  G Loss  0.004  duration: 0.291420\n",
      "Epoch   234:  D Loss  0.004  G Loss  0.004  duration: 0.286173\n",
      "Epoch   235:  D Loss  0.004  G Loss  0.004  duration: 0.289226\n",
      "Epoch   236:  D Loss  0.004  G Loss  0.004  duration: 0.287134\n",
      "Epoch   237:  D Loss  0.004  G Loss  0.004  duration: 0.287236\n",
      "Epoch   238:  D Loss  0.004  G Loss  0.004  duration: 0.287992\n",
      "Epoch   239:  D Loss  0.004  G Loss  0.004  duration: 0.286235\n",
      "Epoch   240:  D Loss  0.004  G Loss  0.004  duration: 0.284180\n",
      "Epoch   241:  D Loss  0.004  G Loss  0.004  duration: 0.291221\n",
      "Epoch   242:  D Loss  0.004  G Loss  0.004  duration: 0.296421\n",
      "Epoch   243:  D Loss  0.004  G Loss  0.004  duration: 0.304185\n",
      "Epoch   244:  D Loss  0.004  G Loss  0.004  duration: 0.283306\n",
      "Epoch   245:  D Loss  0.004  G Loss  0.004  duration: 0.289650\n",
      "Epoch   246:  D Loss  0.004  G Loss  0.004  duration: 0.291222\n",
      "Epoch   247:  D Loss  0.004  G Loss  0.004  duration: 0.286234\n",
      "Epoch   248:  D Loss  0.004  G Loss  0.004  duration: 0.291691\n",
      "Epoch   249:  D Loss  0.004  G Loss  0.004  duration: 0.287657\n",
      "Epoch   250:  D Loss  0.004  G Loss  0.004  duration: 0.290130\n",
      "Epoch   251:  D Loss  0.004  G Loss  0.004  duration: 0.298502\n",
      "Epoch   252:  D Loss  0.004  G Loss  0.004  duration: 0.302266\n",
      "Epoch   253:  D Loss  0.004  G Loss  0.004  duration: 0.282278\n",
      "Epoch   254:  D Loss  0.004  G Loss  0.004  duration: 0.287143\n",
      "Epoch   255:  D Loss  0.004  G Loss  0.004  duration: 0.289227\n",
      "Epoch   256:  D Loss  0.004  G Loss  0.004  duration: 0.289152\n",
      "Epoch   257:  D Loss  0.004  G Loss  0.004  duration: 0.291081\n",
      "Epoch   258:  D Loss  0.004  G Loss  0.004  duration: 0.292330\n",
      "Epoch   259:  D Loss  0.004  G Loss  0.004  duration: 0.291063\n",
      "Epoch   260:  D Loss  0.004  G Loss  0.004  duration: 0.288291\n",
      "Epoch   261:  D Loss  0.004  G Loss  0.004  duration: 0.286235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   262:  D Loss  0.004  G Loss  0.004  duration: 0.294011\n",
      "Epoch   263:  D Loss  0.004  G Loss  0.004  duration: 0.299491\n",
      "Epoch   264:  D Loss  0.004  G Loss  0.004  duration: 0.319579\n",
      "Epoch   265:  D Loss  0.004  G Loss  0.004  duration: 0.311388\n",
      "Epoch   266:  D Loss  0.004  G Loss  0.004  duration: 0.290977\n",
      "Epoch   267:  D Loss  0.004  G Loss  0.004  duration: 0.288082\n",
      "Epoch   268:  D Loss  0.004  G Loss  0.004  duration: 0.290050\n",
      "Epoch   269:  D Loss  0.004  G Loss  0.004  duration: 0.287780\n",
      "Epoch   270:  D Loss  0.004  G Loss  0.004  duration: 0.285045\n",
      "Epoch   271:  D Loss  0.004  G Loss  0.004  duration: 0.283221\n",
      "Epoch   272:  D Loss  0.004  G Loss  0.004  duration: 0.283968\n",
      "Epoch   273:  D Loss  0.004  G Loss  0.004  duration: 0.288416\n",
      "Epoch   274:  D Loss  0.004  G Loss  0.004  duration: 0.288271\n",
      "Epoch   275:  D Loss  0.004  G Loss  0.004  duration: 0.298203\n",
      "Epoch   276:  D Loss  0.004  G Loss  0.004  duration: 0.283803\n",
      "Epoch   277:  D Loss  0.004  G Loss  0.004  duration: 0.284068\n",
      "Epoch   278:  D Loss  0.004  G Loss  0.004  duration: 0.288511\n",
      "Epoch   279:  D Loss  0.004  G Loss  0.004  duration: 0.293204\n",
      "Epoch   280:  D Loss  0.004  G Loss  0.004  duration: 0.287590\n",
      "Epoch   281:  D Loss  0.004  G Loss  0.004  duration: 0.292223\n",
      "Epoch   282:  D Loss  0.004  G Loss  0.004  duration: 0.288171\n",
      "Epoch   283:  D Loss  0.004  G Loss  0.004  duration: 0.284560\n",
      "Epoch   284:  D Loss  0.004  G Loss  0.003  duration: 0.327038\n",
      "Epoch   285:  D Loss  0.003  G Loss  0.003  duration: 0.295149\n",
      "Epoch   286:  D Loss  0.003  G Loss  0.003  duration: 0.288934\n",
      "Epoch   287:  D Loss  0.003  G Loss  0.003  duration: 0.284628\n",
      "Epoch   288:  D Loss  0.003  G Loss  0.003  duration: 0.284275\n",
      "Epoch   289:  D Loss  0.003  G Loss  0.003  duration: 0.293193\n",
      "Epoch   290:  D Loss  0.003  G Loss  0.003  duration: 0.285237\n",
      "Epoch   291:  D Loss  0.003  G Loss  0.003  duration: 0.292336\n",
      "Epoch   292:  D Loss  0.003  G Loss  0.003  duration: 0.291226\n",
      "Epoch   293:  D Loss  0.003  G Loss  0.003  duration: 0.312102\n",
      "Epoch   294:  D Loss  0.003  G Loss  0.003  duration: 0.290596\n",
      "Epoch   295:  D Loss  0.003  G Loss  0.003  duration: 0.292219\n",
      "Epoch   296:  D Loss  0.003  G Loss  0.003  duration: 0.291587\n",
      "Epoch   297:  D Loss  0.003  G Loss  0.003  duration: 0.292200\n",
      "Epoch   298:  D Loss  0.003  G Loss  0.003  duration: 0.283247\n",
      "Epoch   299:  D Loss  0.003  G Loss  0.003  duration: 0.288230\n",
      "Epoch   300:  D Loss  0.003  G Loss  0.003  duration: 0.286188\n",
      "Epoch   301:  D Loss  0.003  G Loss  0.003  duration: 0.288187\n",
      "Epoch   302:  D Loss  0.003  G Loss  0.003  duration: 0.290223\n",
      "Epoch   303:  D Loss  0.003  G Loss  0.003  duration: 0.290224\n",
      "Epoch   304:  D Loss  0.003  G Loss  0.003  duration: 0.301195\n",
      "Epoch   305:  D Loss  0.003  G Loss  0.003  duration: 0.295216\n",
      "Epoch   306:  D Loss  0.003  G Loss  0.003  duration: 0.321151\n",
      "Epoch   307:  D Loss  0.003  G Loss  0.003  duration: 0.353062\n",
      "Epoch   308:  D Loss  0.003  G Loss  0.003  duration: 0.355250\n",
      "Epoch   309:  D Loss  0.003  G Loss  0.003  duration: 0.313142\n",
      "Epoch   310:  D Loss  0.003  G Loss  0.003  duration: 0.298465\n",
      "Epoch   311:  D Loss  0.003  G Loss  0.003  duration: 0.294213\n",
      "Epoch   312:  D Loss  0.003  G Loss  0.003  duration: 0.302192\n",
      "Epoch   313:  D Loss  0.003  G Loss  0.003  duration: 0.315037\n",
      "Epoch   314:  D Loss  0.003  G Loss  0.003  duration: 0.298043\n",
      "Epoch   315:  D Loss  0.003  G Loss  0.003  duration: 0.293220\n",
      "Epoch   316:  D Loss  0.003  G Loss  0.003  duration: 0.301195\n",
      "Epoch   317:  D Loss  0.003  G Loss  0.003  duration: 0.299332\n",
      "Epoch   318:  D Loss  0.003  G Loss  0.003  duration: 0.301108\n",
      "Epoch   319:  D Loss  0.003  G Loss  0.003  duration: 0.307178\n",
      "Epoch   320:  D Loss  0.003  G Loss  0.003  duration: 0.324134\n",
      "Epoch   321:  D Loss  0.003  G Loss  0.003  duration: 0.310398\n",
      "Epoch   322:  D Loss  0.003  G Loss  0.003  duration: 0.315163\n",
      "Epoch   323:  D Loss  0.003  G Loss  0.003  duration: 0.318150\n",
      "Epoch   324:  D Loss  0.003  G Loss  0.003  duration: 0.325131\n",
      "Epoch   325:  D Loss  0.003  G Loss  0.003  duration: 0.334550\n",
      "Epoch   326:  D Loss  0.003  G Loss  0.003  duration: 0.318022\n",
      "Epoch   327:  D Loss  0.003  G Loss  0.003  duration: 0.293618\n",
      "Epoch   328:  D Loss  0.003  G Loss  0.003  duration: 0.291168\n",
      "Epoch   329:  D Loss  0.003  G Loss  0.003  duration: 0.309597\n",
      "Epoch   330:  D Loss  0.003  G Loss  0.003  duration: 0.331120\n",
      "Epoch   331:  D Loss  0.003  G Loss  0.003  duration: 0.329120\n",
      "Epoch   332:  D Loss  0.003  G Loss  0.003  duration: 0.301302\n",
      "Epoch   333:  D Loss  0.003  G Loss  0.003  duration: 0.308176\n",
      "Epoch   334:  D Loss  0.003  G Loss  0.003  duration: 0.298207\n",
      "Epoch   335:  D Loss  0.003  G Loss  0.003  duration: 0.296211\n",
      "Epoch   336:  D Loss  0.003  G Loss  0.003  duration: 0.300197\n",
      "Epoch   337:  D Loss  0.003  G Loss  0.003  duration: 0.301327\n",
      "Epoch   338:  D Loss  0.003  G Loss  0.003  duration: 0.305184\n",
      "Epoch   339:  D Loss  0.003  G Loss  0.003  duration: 0.296830\n",
      "Epoch   340:  D Loss  0.003  G Loss  0.003  duration: 0.305655\n",
      "Epoch   341:  D Loss  0.003  G Loss  0.003  duration: 0.304186\n",
      "Epoch   342:  D Loss  0.003  G Loss  0.003  duration: 0.296138\n",
      "Epoch   343:  D Loss  0.003  G Loss  0.003  duration: 0.335180\n",
      "Epoch   344:  D Loss  0.003  G Loss  0.003  duration: 0.311846\n",
      "Epoch   345:  D Loss  0.003  G Loss  0.003  duration: 0.306323\n",
      "Epoch   346:  D Loss  0.003  G Loss  0.003  duration: 0.298683\n",
      "Epoch   347:  D Loss  0.003  G Loss  0.003  duration: 0.300359\n",
      "Epoch   348:  D Loss  0.003  G Loss  0.003  duration: 0.301655\n",
      "Epoch   349:  D Loss  0.003  G Loss  0.003  duration: 0.302517\n",
      "Epoch   350:  D Loss  0.003  G Loss  0.003  duration: 0.299621\n",
      "Epoch   351:  D Loss  0.003  G Loss  0.003  duration: 0.301742\n",
      "Epoch   352:  D Loss  0.003  G Loss  0.003  duration: 0.301273\n",
      "Epoch   353:  D Loss  0.003  G Loss  0.003  duration: 0.300704\n",
      "Epoch   354:  D Loss  0.003  G Loss  0.003  duration: 0.301527\n",
      "Epoch   355:  D Loss  0.003  G Loss  0.003  duration: 0.298207\n",
      "Epoch   356:  D Loss  0.003  G Loss  0.003  duration: 0.298202\n",
      "Epoch   357:  D Loss  0.003  G Loss  0.003  duration: 0.298203\n",
      "Epoch   358:  D Loss  0.003  G Loss  0.003  duration: 0.298891\n",
      "Epoch   359:  D Loss  0.003  G Loss  0.003  duration: 0.295210\n",
      "Epoch   360:  D Loss  0.003  G Loss  0.003  duration: 0.301199\n",
      "Epoch   361:  D Loss  0.003  G Loss  0.003  duration: 0.299786\n",
      "Epoch   362:  D Loss  0.003  G Loss  0.003  duration: 0.302685\n",
      "Epoch   363:  D Loss  0.003  G Loss  0.003  duration: 0.293215\n",
      "Epoch   364:  D Loss  0.003  G Loss  0.003  duration: 0.296032\n",
      "Epoch   365:  D Loss  0.003  G Loss  0.003  duration: 0.302699\n",
      "Epoch   366:  D Loss  0.003  G Loss  0.003  duration: 0.296208\n",
      "Epoch   367:  D Loss  0.003  G Loss  0.003  duration: 0.301700\n",
      "Epoch   368:  D Loss  0.003  G Loss  0.003  duration: 0.297711\n",
      "Epoch   369:  D Loss  0.003  G Loss  0.003  duration: 0.290781\n",
      "Epoch   370:  D Loss  0.003  G Loss  0.003  duration: 0.324641\n",
      "Epoch   371:  D Loss  0.003  G Loss  0.003  duration: 0.297205\n",
      "Epoch   372:  D Loss  0.003  G Loss  0.003  duration: 0.316661\n",
      "Epoch   373:  D Loss  0.003  G Loss  0.003  duration: 0.299705\n",
      "Epoch   374:  D Loss  0.003  G Loss  0.003  duration: 0.297726\n",
      "Epoch   375:  D Loss  0.003  G Loss  0.003  duration: 0.294721\n",
      "Epoch   376:  D Loss  0.003  G Loss  0.003  duration: 0.297206\n",
      "Epoch   377:  D Loss  0.003  G Loss  0.003  duration: 0.295734\n",
      "Epoch   378:  D Loss  0.003  G Loss  0.003  duration: 0.305185\n",
      "Epoch   379:  D Loss  0.003  G Loss  0.003  duration: 0.300516\n",
      "Epoch   380:  D Loss  0.003  G Loss  0.003  duration: 0.295218\n",
      "Epoch   381:  D Loss  0.003  G Loss  0.003  duration: 0.298208\n",
      "Epoch   382:  D Loss  0.003  G Loss  0.003  duration: 0.299495\n",
      "Epoch   383:  D Loss  0.003  G Loss  0.003  duration: 0.301195\n",
      "Epoch   384:  D Loss  0.003  G Loss  0.003  duration: 0.295147\n",
      "Epoch   385:  D Loss  0.003  G Loss  0.003  duration: 0.294214\n",
      "Epoch   386:  D Loss  0.003  G Loss  0.003  duration: 0.298207\n",
      "Epoch   387:  D Loss  0.003  G Loss  0.003  duration: 0.299209\n",
      "Epoch   388:  D Loss  0.003  G Loss  0.003  duration: 0.299201\n",
      "Epoch   389:  D Loss  0.003  G Loss  0.003  duration: 0.298801\n",
      "Epoch   390:  D Loss  0.003  G Loss  0.002  duration: 0.293134\n",
      "Epoch   391:  D Loss  0.003  G Loss  0.002  duration: 0.305264\n",
      "Epoch   392:  D Loss  0.002  G Loss  0.002  duration: 0.299727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   393:  D Loss  0.002  G Loss  0.002  duration: 0.299647\n",
      "Epoch   394:  D Loss  0.002  G Loss  0.002  duration: 0.293106\n",
      "Epoch   395:  D Loss  0.002  G Loss  0.002  duration: 0.308527\n",
      "Epoch   396:  D Loss  0.002  G Loss  0.002  duration: 0.298208\n",
      "Epoch   397:  D Loss  0.002  G Loss  0.002  duration: 0.300198\n",
      "Epoch   398:  D Loss  0.002  G Loss  0.002  duration: 0.297093\n",
      "Epoch   399:  D Loss  0.002  G Loss  0.002  duration: 0.295214\n",
      "Epoch   400:  D Loss  0.002  G Loss  0.002  duration: 0.298519\n",
      "Epoch   401:  D Loss  0.002  G Loss  0.002  duration: 0.308219\n",
      "Epoch   402:  D Loss  0.002  G Loss  0.002  duration: 0.297494\n",
      "Epoch   403:  D Loss  0.002  G Loss  0.002  duration: 0.336351\n",
      "Epoch   404:  D Loss  0.002  G Loss  0.002  duration: 0.301194\n",
      "Epoch   405:  D Loss  0.002  G Loss  0.002  duration: 0.302702\n",
      "Epoch   406:  D Loss  0.002  G Loss  0.002  duration: 0.303210\n",
      "Epoch   407:  D Loss  0.002  G Loss  0.002  duration: 0.296207\n",
      "Epoch   408:  D Loss  0.002  G Loss  0.002  duration: 0.295211\n",
      "Epoch   409:  D Loss  0.002  G Loss  0.002  duration: 0.302702\n",
      "Epoch   410:  D Loss  0.002  G Loss  0.002  duration: 0.302193\n",
      "Epoch   411:  D Loss  0.002  G Loss  0.002  duration: 0.321141\n",
      "Epoch   412:  D Loss  0.002  G Loss  0.002  duration: 0.304186\n",
      "Epoch   413:  D Loss  0.002  G Loss  0.002  duration: 0.304709\n",
      "Epoch   414:  D Loss  0.002  G Loss  0.002  duration: 0.314957\n",
      "Epoch   415:  D Loss  0.002  G Loss  0.002  duration: 0.299221\n",
      "Epoch   416:  D Loss  0.002  G Loss  0.002  duration: 0.311168\n",
      "Epoch   417:  D Loss  0.002  G Loss  0.002  duration: 0.328632\n",
      "Epoch   418:  D Loss  0.002  G Loss  0.002  duration: 0.304187\n",
      "Epoch   419:  D Loss  0.002  G Loss  0.002  duration: 0.296346\n",
      "Epoch   420:  D Loss  0.002  G Loss  0.002  duration: 0.301220\n",
      "Epoch   421:  D Loss  0.002  G Loss  0.002  duration: 0.296208\n",
      "Epoch   422:  D Loss  0.002  G Loss  0.002  duration: 0.299200\n",
      "Epoch   423:  D Loss  0.002  G Loss  0.002  duration: 0.302698\n",
      "Epoch   424:  D Loss  0.002  G Loss  0.002  duration: 0.300704\n",
      "Epoch   425:  D Loss  0.002  G Loss  0.002  duration: 0.294233\n",
      "Epoch   426:  D Loss  0.002  G Loss  0.002  duration: 0.305184\n",
      "Epoch   427:  D Loss  0.002  G Loss  0.002  duration: 0.296715\n",
      "Epoch   428:  D Loss  0.002  G Loss  0.002  duration: 0.310171\n",
      "Epoch   429:  D Loss  0.002  G Loss  0.002  duration: 0.298220\n",
      "Epoch   430:  D Loss  0.002  G Loss  0.002  duration: 0.323136\n",
      "Epoch   431:  D Loss  0.002  G Loss  0.002  duration: 0.302191\n",
      "Epoch   432:  D Loss  0.002  G Loss  0.002  duration: 0.299200\n",
      "Epoch   433:  D Loss  0.002  G Loss  0.002  duration: 0.300198\n",
      "Epoch   434:  D Loss  0.002  G Loss  0.002  duration: 0.301212\n",
      "Epoch   435:  D Loss  0.002  G Loss  0.002  duration: 0.299201\n",
      "Epoch   436:  D Loss  0.002  G Loss  0.002  duration: 0.302192\n",
      "Epoch   437:  D Loss  0.002  G Loss  0.002  duration: 0.299710\n",
      "Epoch   438:  D Loss  0.002  G Loss  0.002  duration: 0.305184\n",
      "Epoch   439:  D Loss  0.002  G Loss  0.002  duration: 0.297730\n",
      "Epoch   440:  D Loss  0.002  G Loss  0.002  duration: 0.298203\n",
      "Epoch   441:  D Loss  0.002  G Loss  0.002  duration: 0.303190\n",
      "Epoch   442:  D Loss  0.002  G Loss  0.002  duration: 0.297221\n",
      "Epoch   443:  D Loss  0.002  G Loss  0.002  duration: 0.303212\n",
      "Epoch   444:  D Loss  0.002  G Loss  0.002  duration: 0.303190\n",
      "Epoch   445:  D Loss  0.002  G Loss  0.002  duration: 0.297363\n",
      "Epoch   446:  D Loss  0.002  G Loss  0.002  duration: 0.299205\n",
      "Epoch   447:  D Loss  0.002  G Loss  0.002  duration: 0.305188\n",
      "Epoch   448:  D Loss  0.002  G Loss  0.002  duration: 0.296209\n",
      "Epoch   449:  D Loss  0.002  G Loss  0.002  duration: 0.294411\n",
      "Epoch   450:  D Loss  0.002  G Loss  0.002  duration: 0.304186\n",
      "Epoch   451:  D Loss  0.002  G Loss  0.002  duration: 0.295906\n",
      "Epoch   452:  D Loss  0.002  G Loss  0.002  duration: 0.294205\n",
      "Epoch   453:  D Loss  0.002  G Loss  0.002  duration: 0.301883\n",
      "Epoch   454:  D Loss  0.002  G Loss  0.002  duration: 0.295645\n",
      "Epoch   455:  D Loss  0.002  G Loss  0.002  duration: 0.294989\n",
      "Epoch   456:  D Loss  0.002  G Loss  0.002  duration: 0.296297\n",
      "Epoch   457:  D Loss  0.002  G Loss  0.002  duration: 0.297206\n",
      "Epoch   458:  D Loss  0.002  G Loss  0.002  duration: 0.292217\n",
      "Epoch   459:  D Loss  0.002  G Loss  0.002  duration: 0.302336\n",
      "Epoch   460:  D Loss  0.002  G Loss  0.002  duration: 0.294343\n",
      "Epoch   461:  D Loss  0.002  G Loss  0.002  duration: 0.306759\n",
      "Epoch   462:  D Loss  0.002  G Loss  0.002  duration: 0.301194\n",
      "Epoch   463:  D Loss  0.002  G Loss  0.002  duration: 0.324133\n",
      "Epoch   464:  D Loss  0.002  G Loss  0.002  duration: 0.344096\n",
      "Epoch   465:  D Loss  0.002  G Loss  0.002  duration: 0.302208\n",
      "Epoch   466:  D Loss  0.002  G Loss  0.002  duration: 0.302192\n",
      "Epoch   467:  D Loss  0.002  G Loss  0.002  duration: 0.294214\n",
      "Epoch   468:  D Loss  0.002  G Loss  0.002  duration: 0.296208\n",
      "Epoch   469:  D Loss  0.002  G Loss  0.002  duration: 0.306181\n",
      "Epoch   470:  D Loss  0.002  G Loss  0.002  duration: 0.301212\n",
      "Epoch   471:  D Loss  0.002  G Loss  0.002  duration: 0.297205\n",
      "Epoch   472:  D Loss  0.002  G Loss  0.002  duration: 0.294213\n",
      "Epoch   473:  D Loss  0.002  G Loss  0.002  duration: 0.301700\n",
      "Epoch   474:  D Loss  0.002  G Loss  0.002  duration: 0.289731\n",
      "Epoch   475:  D Loss  0.002  G Loss  0.002  duration: 0.298219\n",
      "Epoch   476:  D Loss  0.002  G Loss  0.002  duration: 0.304187\n",
      "Epoch   477:  D Loss  0.002  G Loss  0.002  duration: 0.306182\n",
      "Epoch   478:  D Loss  0.002  G Loss  0.002  duration: 0.332645\n",
      "Epoch   479:  D Loss  0.002  G Loss  0.002  duration: 0.331135\n",
      "Epoch   480:  D Loss  0.002  G Loss  0.002  duration: 0.311169\n",
      "Epoch   481:  D Loss  0.002  G Loss  0.002  duration: 0.321647\n",
      "Epoch   482:  D Loss  0.002  G Loss  0.002  duration: 0.322587\n",
      "Epoch   483:  D Loss  0.002  G Loss  0.002  duration: 0.336625\n",
      "Epoch   484:  D Loss  0.002  G Loss  0.002  duration: 0.290223\n",
      "Epoch   485:  D Loss  0.002  G Loss  0.002  duration: 0.299200\n",
      "Epoch   486:  D Loss  0.002  G Loss  0.002  duration: 0.308186\n",
      "Epoch   487:  D Loss  0.002  G Loss  0.002  duration: 0.350212\n",
      "Epoch   488:  D Loss  0.002  G Loss  0.002  duration: 0.344080\n",
      "Epoch   489:  D Loss  0.002  G Loss  0.002  duration: 0.316056\n",
      "Epoch   490:  D Loss  0.002  G Loss  0.002  duration: 0.294213\n",
      "Epoch   491:  D Loss  0.002  G Loss  0.002  duration: 0.315158\n",
      "Epoch   492:  D Loss  0.002  G Loss  0.002  duration: 0.319579\n",
      "Epoch   493:  D Loss  0.002  G Loss  0.002  duration: 0.303003\n",
      "Epoch   494:  D Loss  0.002  G Loss  0.002  duration: 0.300332\n",
      "Epoch   495:  D Loss  0.002  G Loss  0.002  duration: 0.311173\n",
      "Epoch   496:  D Loss  0.002  G Loss  0.002  duration: 0.310171\n",
      "Epoch   497:  D Loss  0.002  G Loss  0.002  duration: 0.318155\n",
      "Epoch   498:  D Loss  0.002  G Loss  0.002  duration: 0.379255\n",
      "Epoch   499:  D Loss  0.002  G Loss  0.002  duration: 0.308544\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 500\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    G.train()\n",
    "    D.train()\n",
    "    \n",
    "    sum_real_error = 0\n",
    "    sum_gen_loss = 0\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for idx in range(0, len(xtrain)-batch_size+1, batch_size):\n",
    "        \n",
    "        \"\"\"\n",
    "            训练Discriminator\n",
    "        \"\"\"\n",
    "        D.zero_grad()\n",
    "        # 用真实样本训练D \n",
    "        real_data = get_real_samples(idx, batch_size)\n",
    "        real_decision = D(real_data)\n",
    "        real_error = -torch.sum(torch.log(real_decision))/batch_size\n",
    "        sum_real_error += real_error\n",
    "        real_error.backward()\n",
    "    \n",
    "        # 用生成样本训练D\n",
    "        input_data = get_noise_data(batch_size, input_size)\n",
    "        fake_data = G.forward(input_data) \n",
    "        fake_decision = D(fake_data)\n",
    "        fake_error = 1 - torch.sum(torch.log(fake_decision))/batch_size\n",
    "        fake_error.backward()\n",
    "    \n",
    "        d_optimizer.step()\n",
    "    \n",
    "    \n",
    "        \"\"\"\n",
    "            训练Generator\n",
    "        \"\"\"\n",
    "        G.zero_grad()\n",
    "        # 训练G\n",
    "        input_data = get_noise_data(batch_size, input_size)\n",
    "        fake_data = G.forward(input_data)\n",
    "        fake_decision = D(fake_data)\n",
    "        gen_loss = -torch.sum(torch.log(fake_decision))/batch_size\n",
    "        sum_gen_loss += gen_loss\n",
    "        gen_loss.backward()\n",
    "\n",
    "        g_optimizer.step()\n",
    "    \n",
    "    print(\"Epoch %5d:  D Loss  %5.3f  \" % (epoch, sum_real_error), end=\"\")\n",
    "    print(\"G Loss  %5.3f  \" % (sum_gen_loss), end=\"\")\n",
    "    print(\"duration: %5f\" %(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 利用Generator生成时间序列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_batch_size = 100\n",
    "\n",
    "input_data = get_noise_data(gen_batch_size, input_size)\n",
    "time_series = G.forward(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series = time_series.view(gen_batch_size, -1, 4).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 26, 4)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_series = time_series * (xtrain_max - xtrain_min) + xtrain_mean\n",
    "time_series.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./time_series.npy', time_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
