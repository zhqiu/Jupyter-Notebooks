{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAN 和 Recurrent GAN 的结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GAN:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](./GAN.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 利用GAN生成历史数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](./stock_data_gen.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 利用MLP产生一组随机的时间序列（Generator）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.normal import Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 产生用于输入Generator的噪声, 为正态分布\n",
    "# 其大小为 m x n\n",
    "\n",
    "def get_noise_data(m, n):\n",
    "    dist = Normal(0, 1)\n",
    "    return dist.sample((m, n)).requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_size=20, num_features=4, batch_size=10, seq_len=26):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size   # nums of input rand numbers\n",
    "        self.num_features = num_features\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_len = seq_len\n",
    "        self.output_size = self.num_features * self.seq_len\n",
    "        \n",
    "        # 使用MLP\n",
    "        self.fc1 = nn.Linear(self.input_size, self.output_size*10)\n",
    "        self.fc2 = nn.Linear(self.output_size*10, self.output_size*5)\n",
    "        self.fc3 = nn.Linear(self.output_size*5, self.output_size)\n",
    "        \n",
    "    def forward(self, input_data):\n",
    "        output = torch.sigmoid(self.fc1(input_data))\n",
    "        output = torch.sigmoid(self.fc2(output))\n",
    "        output = torch.sigmoid(self.fc3(output))\n",
    "        \n",
    "        # output size: [batch_size, channels=num_features, width=seq_len]\n",
    "        output = output.view(output.size(0), self.num_features, self.seq_len)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 20\n",
    "batch_size = 10\n",
    "seq_len = 26\n",
    "\n",
    "g = Generator(input_size=input_size, batch_size=batch_size, seq_len=seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 4, 26])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data = get_noise_data(batch_size, input_size)\n",
    "\n",
    "time_series = g.forward(input_data)\n",
    "\n",
    "time_series.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 下面是利用CNN构造的Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, batch_size=10, seq_len=26, num_features=4):\n",
    "        super().__init__()\n",
    "        self.input_size = seq_len * num_features\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        # 使用 CNN\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=4, out_channels=8, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=8, out_channels=16, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(640, 256),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 1)\n",
    "        \n",
    "    def forward(self, input_data):\n",
    "        # input_data的大小：[batch_size, channels=4, width=26]\n",
    "        output = self.conv1(input_data)\n",
    "        output = self.conv2(output)\n",
    "        output = self.conv3(output)\n",
    "        output = output.view(output.size(0), -1) # [batch_size, 640]\n",
    "        output = self.fc1(output)\n",
    "        output = self.fc2(output)\n",
    "        output = self.fc3(output)\n",
    "        output = torch.sigmoid(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 4, 26])\n",
      "tensor([[0.4926],\n",
      "        [0.4927],\n",
      "        [0.4926],\n",
      "        [0.4926],\n",
      "        [0.4926],\n",
      "        [0.4926],\n",
      "        [0.4926],\n",
      "        [0.4926],\n",
      "        [0.4927],\n",
      "        [0.4926]], grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "# 一起测试Generator和Discriminator\n",
    "input_size = 20\n",
    "batch_size = 10\n",
    "seq_len = 26\n",
    "\n",
    "g = Generator(input_size=input_size, batch_size=batch_size, seq_len=seq_len)\n",
    "input_data = get_noise_data(batch_size, input_size)\n",
    "time_series = g.forward(input_data)\n",
    "\n",
    "print(time_series.shape)\n",
    "\n",
    "d = Discriminator(batch_size=batch_size, seq_len=seq_len)\n",
    "outputs = d.forward(time_series)\n",
    "\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 获取真实样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从csv文件中读取数据，将数据划分为训练集和测试集\n",
    "# 再从训练集中随机抽取m个长度为n的串返回\n",
    "\n",
    "data = pd.read_csv('stock_data_730.csv')\n",
    "\n",
    "data.set_index([\"date\"], inplace=True)\n",
    "data_sorted = data.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将所有数据分割为训练集和测试集\n",
    "# 与 LSTM -- predict stock price中的函数相同\n",
    "\n",
    "def train_test_split(data, SEQ_LENGTH = 26, test_prop=0.137):  # 0.11 for 1095, 0.137 for 730, 0.3 for 365\n",
    "    \n",
    "    ntrain = int(len(data) *(1-test_prop))  # len(data) = 197\n",
    "    predictors = data.columns[:4]  # open, high, close, low\n",
    "    data_pred = data[predictors]\n",
    "    num_attr = data_pred.shape[1]  # 4\n",
    "    \n",
    "    result = np.empty((len(data) - SEQ_LENGTH, SEQ_LENGTH, num_attr))\n",
    "    y = np.empty((len(data) - SEQ_LENGTH, SEQ_LENGTH))\n",
    "    yopen = np.empty((len(data) - SEQ_LENGTH, SEQ_LENGTH))\n",
    "\n",
    "    for index in range(len(data) - SEQ_LENGTH):\n",
    "        result[index, :, :] = data_pred[index: index + SEQ_LENGTH]\n",
    "        y[index, :] = data_pred[index+1: index + SEQ_LENGTH + 1].close\n",
    "        yopen[index, :] = data_pred[index+1: index + SEQ_LENGTH + 1].open\n",
    "\n",
    "    \"\"\"\n",
    "        xtrain的大小：ntrain x SEQ_LENGTH x 4\n",
    "        ytrain的大小：ntrain x SEQ_LENGTH\n",
    "        \n",
    "        * xtrain的每个batch为长为SEQ_LENGTH的连续序列，一共有ntrain个batch，\n",
    "          序列中每个单元都是一个四元组（open，high，close，low）\n",
    "        * ytrain的每个batch为长为SEQ_LENGTH的连续序列，一共有ntrain个batch，\n",
    "          序列中每个单元是xtrain中对应四元组所在日期的下一天的close price\n",
    "        \n",
    "        xtest 的大小：    ntest x SEQ_LENGTH x 4                \n",
    "        ytest的大小：     ntest x SEQ_LENGTH      (close price)\n",
    "        ytest_open的大小：ntest x SEQ_LENGTH      (open price)  \n",
    "        \n",
    "        * xtest的每个batch为长为SEQ_LENGTH的连续序列，一共有ntest个batch，\n",
    "          序列中每个单元都是一个四元组（open，high，close，low）\n",
    "          每一个序列仅包含一个新四元组，且在最后一个\n",
    "        * ytest的每个batch为长为SEQ_LENGTH的连续序列，一共有ntest个batch，\n",
    "          序列中每个单元是xtest中对应四元组所在日期的下一天的close price\n",
    "        \n",
    "        类型：numpy.ndarray\n",
    "    \"\"\"\n",
    "    xtrain = result[:ntrain, :, :]\n",
    "    ytrain = y[:ntrain]\n",
    "    \n",
    "    xtest = result[ntrain:, :, :]\n",
    "    ytest = y[ntrain:]\n",
    "    ytest_open = yopen[ntrain:]\n",
    "    \n",
    "    return xtrain, xtest, ytrain, ytest, ytest_open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xtest, ytrain, ytest, ytest_open = train_test_split(data_sorted)  # 只需要xtrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.418522797766748 21.26 9.86\n"
     ]
    }
   ],
   "source": [
    "xtrain.shape  # open, high, close, low\n",
    "\n",
    "xtrain_mean = np.mean(xtrain)\n",
    "xtrain_max = np.max(xtrain)\n",
    "xtrain_min = np.min(xtrain)\n",
    "\n",
    "print(xtrain_mean, xtrain_max, xtrain_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取下标从start_idx开始的连续batch_size个序列\n",
    "\n",
    "def get_real_samples(idx, batch_size, data=xtrain):\n",
    "    data = data[idx:idx+batch_size, :]\n",
    "    min_val = np.min(data)\n",
    "    max_val = np.max(data)\n",
    "    mean_val = np.mean(data)\n",
    "    data = (data-mean_val)/(max_val-min_val)\n",
    "    \n",
    "    data = torch.from_numpy(data).float()\n",
    "    data = data.view(batch_size, 4, -1)\n",
    "    \n",
    "    return data.requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 26])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_real_samples(0, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 4, 26])\n",
      "tensor([[0.4988],\n",
      "        [0.4987],\n",
      "        [0.4987],\n",
      "        [0.4986],\n",
      "        [0.4987],\n",
      "        [0.4988],\n",
      "        [0.4990],\n",
      "        [0.4990],\n",
      "        [0.4990],\n",
      "        [0.4989]], grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "# 将real samples带入Discriminator中进行测试\n",
    "\n",
    "batch_size = 10\n",
    "seq_len = 26\n",
    "\n",
    "real_samples = get_real_samples(0, batch_size)\n",
    "\n",
    "print(real_samples.shape)\n",
    "\n",
    "d = Discriminator(batch_size=batch_size, seq_len=seq_len)\n",
    "outputs = d.forward(real_samples)\n",
    "\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构造GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_learning_rate = 0.01\n",
    "g_learning_rate = 0.01\n",
    "\n",
    "input_size = 20\n",
    "batch_size = 5\n",
    "seq_len = 26\n",
    "\n",
    "G = Generator(input_size=input_size, batch_size=batch_size, seq_len=seq_len)\n",
    "D = Discriminator(batch_size=batch_size, seq_len=seq_len)\n",
    "\n",
    "d_optimizer = optim.SGD(D.parameters(), lr=d_learning_rate) \n",
    "g_optimizer = optim.SGD(G.parameters(), lr=g_learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch     0:  D Loss  31.349  G Loss  30.800  duration: 1.187165\n",
      "Epoch     1:  D Loss  8.023  G Loss  7.283  duration: 0.553260\n",
      "Epoch     2:  D Loss  1.480  G Loss  0.972  duration: 0.531216\n",
      "Epoch     3:  D Loss  0.567  G Loss  0.287  duration: 0.561081\n",
      "Epoch     4:  D Loss  0.318  G Loss  0.136  duration: 0.613552\n",
      "Epoch     5:  D Loss  0.212  G Loss  0.081  duration: 0.584287\n",
      "Epoch     6:  D Loss  0.155  G Loss  0.054  duration: 0.578336\n",
      "Epoch     7:  D Loss  0.121  G Loss  0.039  duration: 0.561472\n",
      "Epoch     8:  D Loss  0.098  G Loss  0.030  duration: 0.561109\n",
      "Epoch     9:  D Loss  0.082  G Loss  0.024  duration: 0.503439\n",
      "Epoch    10:  D Loss  0.070  G Loss  0.019  duration: 0.506416\n",
      "Epoch    11:  D Loss  0.060  G Loss  0.016  duration: 0.575361\n",
      "Epoch    12:  D Loss  0.053  G Loss  0.014  duration: 0.553040\n",
      "Epoch    13:  D Loss  0.047  G Loss  0.012  duration: 0.531215\n",
      "Epoch    14:  D Loss  0.043  G Loss  0.010  duration: 0.526712\n",
      "Epoch    15:  D Loss  0.039  G Loss  0.009  duration: 0.509889\n",
      "Epoch    16:  D Loss  0.035  G Loss  0.008  duration: 0.502448\n",
      "Epoch    17:  D Loss  0.032  G Loss  0.007  duration: 0.555925\n",
      "Epoch    18:  D Loss  0.030  G Loss  0.007  duration: 0.523280\n",
      "Epoch    19:  D Loss  0.028  G Loss  0.006  duration: 0.650752\n",
      "Epoch    20:  D Loss  0.026  G Loss  0.006  duration: 0.768800\n",
      "Epoch    21:  D Loss  0.024  G Loss  0.005  duration: 0.692912\n",
      "Epoch    22:  D Loss  0.023  G Loss  0.005  duration: 0.684975\n",
      "Epoch    23:  D Loss  0.021  G Loss  0.004  duration: 0.698398\n",
      "Epoch    24:  D Loss  0.020  G Loss  0.004  duration: 0.673070\n",
      "Epoch    25:  D Loss  0.019  G Loss  0.004  duration: 0.711757\n",
      "Epoch    26:  D Loss  0.018  G Loss  0.004  duration: 0.700251\n",
      "Epoch    27:  D Loss  0.017  G Loss  0.003  duration: 0.678119\n",
      "Epoch    28:  D Loss  0.017  G Loss  0.003  duration: 0.664639\n",
      "Epoch    29:  D Loss  0.016  G Loss  0.003  duration: 0.694400\n",
      "Epoch    30:  D Loss  0.015  G Loss  0.003  duration: 0.701344\n",
      "Epoch    31:  D Loss  0.014  G Loss  0.003  duration: 0.682991\n",
      "Epoch    32:  D Loss  0.014  G Loss  0.003  duration: 0.675055\n",
      "Epoch    33:  D Loss  0.013  G Loss  0.002  duration: 0.723664\n",
      "Epoch    34:  D Loss  0.013  G Loss  0.002  duration: 0.704320\n",
      "Epoch    35:  D Loss  0.012  G Loss  0.002  duration: 0.753419\n",
      "Epoch    36:  D Loss  0.012  G Loss  0.002  duration: 0.699360\n",
      "Epoch    37:  D Loss  0.011  G Loss  0.002  duration: 0.697871\n",
      "Epoch    38:  D Loss  0.011  G Loss  0.002  duration: 0.730113\n",
      "Epoch    39:  D Loss  0.011  G Loss  0.002  duration: 0.668112\n",
      "Epoch    40:  D Loss  0.010  G Loss  0.002  duration: 0.663652\n",
      "Epoch    41:  D Loss  0.010  G Loss  0.002  duration: 0.664726\n",
      "Epoch    42:  D Loss  0.010  G Loss  0.002  duration: 0.684480\n",
      "Epoch    43:  D Loss  0.009  G Loss  0.002  duration: 0.712752\n",
      "Epoch    44:  D Loss  0.009  G Loss  0.001  duration: 0.757379\n",
      "Epoch    45:  D Loss  0.009  G Loss  0.001  duration: 0.700338\n",
      "Epoch    46:  D Loss  0.009  G Loss  0.001  duration: 0.641824\n",
      "Epoch    47:  D Loss  0.008  G Loss  0.001  duration: 0.659184\n",
      "Epoch    48:  D Loss  0.008  G Loss  0.001  duration: 0.715717\n",
      "Epoch    49:  D Loss  0.008  G Loss  0.001  duration: 0.726144\n",
      "Epoch    50:  D Loss  0.008  G Loss  0.001  duration: 0.744496\n",
      "Epoch    51:  D Loss  0.008  G Loss  0.001  duration: 0.708287\n",
      "Epoch    52:  D Loss  0.007  G Loss  0.001  duration: 0.659680\n",
      "Epoch    53:  D Loss  0.007  G Loss  0.001  duration: 0.662656\n",
      "Epoch    54:  D Loss  0.007  G Loss  0.001  duration: 0.648289\n",
      "Epoch    55:  D Loss  0.007  G Loss  0.001  duration: 0.662656\n",
      "Epoch    56:  D Loss  0.007  G Loss  0.001  duration: 0.668127\n",
      "Epoch    57:  D Loss  0.007  G Loss  0.001  duration: 0.691423\n",
      "Epoch    58:  D Loss  0.006  G Loss  0.001  duration: 0.701708\n",
      "Epoch    59:  D Loss  0.006  G Loss  0.001  duration: 0.680512\n",
      "Epoch    60:  D Loss  0.006  G Loss  0.001  duration: 0.699856\n",
      "Epoch    61:  D Loss  0.006  G Loss  0.001  duration: 0.693407\n",
      "Epoch    62:  D Loss  0.006  G Loss  0.001  duration: 0.766320\n",
      "Epoch    63:  D Loss  0.006  G Loss  0.001  duration: 0.738049\n",
      "Epoch    64:  D Loss  0.006  G Loss  0.001  duration: 0.680511\n",
      "Epoch    65:  D Loss  0.006  G Loss  0.001  duration: 0.688447\n",
      "Epoch    66:  D Loss  0.005  G Loss  0.001  duration: 0.700352\n",
      "Epoch    67:  D Loss  0.005  G Loss  0.001  duration: 0.778222\n",
      "Epoch    68:  D Loss  0.005  G Loss  0.001  duration: 0.679024\n",
      "Epoch    69:  D Loss  0.005  G Loss  0.001  duration: 0.674560\n",
      "Epoch    70:  D Loss  0.005  G Loss  0.001  duration: 0.697473\n",
      "Epoch    71:  D Loss  0.005  G Loss  0.001  duration: 0.698368\n",
      "Epoch    72:  D Loss  0.005  G Loss  0.001  duration: 0.759365\n",
      "Epoch    73:  D Loss  0.005  G Loss  0.001  duration: 0.714240\n",
      "Epoch    74:  D Loss  0.005  G Loss  0.001  duration: 0.678032\n",
      "Epoch    75:  D Loss  0.005  G Loss  0.001  duration: 0.703327\n",
      "Epoch    76:  D Loss  0.005  G Loss  0.001  duration: 0.682496\n",
      "Epoch    77:  D Loss  0.005  G Loss  0.001  duration: 0.676544\n",
      "Epoch    78:  D Loss  0.004  G Loss  0.001  duration: 0.679024\n",
      "Epoch    79:  D Loss  0.004  G Loss  0.001  duration: 0.727632\n",
      "Epoch    80:  D Loss  0.004  G Loss  0.001  duration: 0.740528\n",
      "Epoch    81:  D Loss  0.004  G Loss  0.001  duration: 0.676048\n",
      "Epoch    82:  D Loss  0.004  G Loss  0.001  duration: 0.691920\n",
      "Epoch    83:  D Loss  0.004  G Loss  0.001  duration: 0.732591\n",
      "Epoch    84:  D Loss  0.004  G Loss  0.001  duration: 0.739536\n",
      "Epoch    85:  D Loss  0.004  G Loss  0.001  duration: 0.692260\n",
      "Epoch    86:  D Loss  0.004  G Loss  0.001  duration: 0.733088\n",
      "Epoch    87:  D Loss  0.004  G Loss  0.000  duration: 0.729120\n",
      "Epoch    88:  D Loss  0.004  G Loss  0.000  duration: 0.699896\n",
      "Epoch    89:  D Loss  0.004  G Loss  0.000  duration: 0.729120\n",
      "Epoch    90:  D Loss  0.004  G Loss  0.000  duration: 0.681008\n",
      "Epoch    91:  D Loss  0.004  G Loss  0.000  duration: 0.761856\n",
      "Epoch    92:  D Loss  0.004  G Loss  0.000  duration: 0.710271\n",
      "Epoch    93:  D Loss  0.004  G Loss  0.000  duration: 0.722673\n",
      "Epoch    94:  D Loss  0.004  G Loss  0.000  duration: 0.710272\n",
      "Epoch    95:  D Loss  0.003  G Loss  0.000  duration: 0.746975\n",
      "Epoch    96:  D Loss  0.003  G Loss  0.000  duration: 0.731104\n",
      "Epoch    97:  D Loss  0.003  G Loss  0.000  duration: 0.768800\n",
      "Epoch    98:  D Loss  0.003  G Loss  0.000  duration: 0.703765\n",
      "Epoch    99:  D Loss  0.003  G Loss  0.000  duration: 0.652241\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    G.train()\n",
    "    D.train()\n",
    "    \n",
    "    sum_real_error = 0\n",
    "    sum_gen_loss = 0\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for idx in range(0, len(xtrain)-batch_size+1, batch_size):\n",
    "        \n",
    "        \"\"\"\n",
    "            训练Discriminator\n",
    "        \"\"\"\n",
    "        D.zero_grad()\n",
    "        # 用真实样本训练D \n",
    "        real_data = get_real_samples(idx, batch_size)\n",
    "        real_decision = D(real_data)\n",
    "        real_error = -torch.sum(torch.log(real_decision))/batch_size\n",
    "        sum_real_error += real_error\n",
    "        real_error.backward()\n",
    "    \n",
    "        # 用生成样本训练D\n",
    "        input_data = get_noise_data(batch_size, input_size)\n",
    "        fake_data = G.forward(input_data) \n",
    "        fake_decision = D(fake_data)\n",
    "        fake_error = 1 - torch.sum(torch.log(fake_decision))/batch_size\n",
    "        fake_error.backward()\n",
    "    \n",
    "        d_optimizer.step()\n",
    "    \n",
    "    \n",
    "        \"\"\"\n",
    "            训练Generator\n",
    "        \"\"\"\n",
    "        G.zero_grad()\n",
    "        # 训练G\n",
    "        input_data = get_noise_data(batch_size, input_size)\n",
    "        fake_data = G.forward(input_data)\n",
    "        fake_decision = D(fake_data)\n",
    "        gen_loss = -torch.sum(torch.log(fake_decision))/batch_size\n",
    "        sum_gen_loss += gen_loss\n",
    "        gen_loss.backward()\n",
    "\n",
    "        g_optimizer.step()\n",
    "    \n",
    "    print(\"Epoch %5d:  D Loss  %5.3f  \" % (epoch, sum_real_error), end=\"\")\n",
    "    print(\"G Loss  %5.3f  \" % (sum_gen_loss), end=\"\")\n",
    "    print(\"duration: %5f\" %(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 利用Generator生成时间序列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 4, 26])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_batch_size = 100\n",
    "\n",
    "input_data = get_noise_data(gen_batch_size, input_size)\n",
    "time_series = G.forward(input_data)\n",
    "\n",
    "time_series.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 26, 4)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_series = time_series.permute(0,2,1).detach().numpy()\n",
    "time_series.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 26, 4)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_series = time_series * (xtrain_max - xtrain_min) + xtrain_mean\n",
    "time_series.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./time_series.npy', time_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
