{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "来源：http://neupy.com/2016/12/17/hyperparameter_optimization_for_neural_networks.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 简介"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "神经网络中一些需要确定的超参数：\n",
    "> 1. Number of layers  \n",
    "> 2. Different parameters for each layer (number of hidden units, filter size for convolutional layer and so on)  \n",
    "> 3. Type of activation functions  \n",
    "> 4. Parameter initialization method  \n",
    "> 5. Learning rate  \n",
    "> 6. Loss function  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一些超参数选择方法： \n",
    "> 1. Grid Search  \n",
    "> 2. Random Search  \n",
    "> 3. Hand-tuning \n",
    "> 4. Gaussian Process with Expected Improvement  \n",
    "> 5. Tree-structured Parzen Estimators (TPE)  \n",
    "\n",
    "其中4，5属于Bayesian optimization。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为每个要确定的参数定义一组值，依次尝试所有的组合。   \n",
    "5个参数，每个参数10个可能值，一共$10^5$种组合。  \n",
    "耗时会很久。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "随机地挑选一些超参数。  \n",
    "但随机挑选的结果将是点的分布有疏有密。  \n",
    "可以利用low-discrepancy sequence缓解这个问题，比如Halton sequence。  \n",
    "但是维度高时，这种方法并不理想。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![p1](./100-uniform-data-points.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![p2](./quasi-random-points.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hand-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "人在调参的过程中可以记住并分析之前的结果，在之后的超参数设置时可以利用这些经验。  \n",
    "在这个角度，hand-tuning比grid search和random search高效，因为后面两种方法没有记忆。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如何存储记忆？有深度的方法和非深度的方法。深度方法比如使用RL，非深度的方法比如下面提到的Bayesian optimization。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将上述过程自动化，可以得到Bayesian optimization。  \n",
    "在Bayesian optimization中可以利用Gaussian Process与Acquisition Function。  \n",
    "Gaussian Process可以利用已有的调参结果建立关于超参数的假设，Acquisition Function可以利用这些知识进行下一轮的选择。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian Process：对每个x有 y=f(x)  \n",
    "f是一个随机函数，它从一个与x有关的高斯分布采样出输出y。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![p3](./guassian-process-1.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "随着采样的不断进行，这个函数会越来越清晰。（下图蓝色区域为95%置信区间）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![p4](./guassian-process-2.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Acquisition Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acquisition function可以获得当前对超参数的分布的估计下的最优解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected Improvement（期望提升）的定义：$g_{max}(x)=max(0,y_{highest}-y_{max})$  \n",
    "$y_{highest}$为当前分布的最高点，  \n",
    "$y_{max}$为观测值最高点。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![p5](./expected-improvement-example.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面的例子中使用基于Gaussian process和expected improvement的Bayesian optimization来寻找最佳的隐层单元数量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下面的函数可以设置隐层单元数量，然后训练网络，返回错误率\n",
    "\n",
    "from neupy import algorithms, layers\n",
    "\n",
    "def train_network(n_hidden, x_train, x_test, y_train, y_test):\n",
    "    network = algorithms.Momentum(\n",
    "        [\n",
    "            layers.Input(64),\n",
    "            layers.Relu(n_hidden),\n",
    "            layers.Softmax(10),\n",
    "        ],\n",
    "\n",
    "        # Randomly shuffle dataset before each\n",
    "        # training epoch.\n",
    "        shuffle_data=True,\n",
    "\n",
    "        # Do not show training progress in output\n",
    "        verbose=False,\n",
    "\n",
    "        step=0.001,\n",
    "        batch_size=128,\n",
    "        error='categorical_crossentropy',\n",
    "    )\n",
    "    network.train(x_train, y_train, epochs=100)\n",
    "\n",
    "    # Calculates categorical cross-entropy error between\n",
    "    # predicted value for x_test and y_test value\n",
    "    return network.prediction_error(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备数据集\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from neupy import environment\n",
    "\n",
    "environment.reproducible()\n",
    "\n",
    "dataset = datasets.load_digits()\n",
    "n_samples = dataset.target.size\n",
    "n_classes = 10\n",
    "\n",
    "# One-hot encoder\n",
    "target = np.zeros((n_samples, n_classes))\n",
    "target[np.arange(n_samples), dataset.target] = 1\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    dataset.data, target, train_size=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下面的函数利用观测到的数据（x_train，y_train）来拟合一个gaussian process\n",
    "# 然后为每个可能的隐层个数生成对应的均值和标准差，然后返回\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "\n",
    "def vector_2d(array):\n",
    "    return np.array(array).reshape((-1, 1))\n",
    "\n",
    "def gaussian_process(x_train, y_train, x_test):\n",
    "    x_train = vector_2d(x_train)\n",
    "    y_train = vector_2d(y_train)\n",
    "    x_test = vector_2d(x_test)\n",
    "\n",
    "    # Train gaussian process\n",
    "    gp = GaussianProcessRegressor(corr='squared_exponential',\n",
    "                         theta0=1e-1, thetaL=1e-3, thetaU=1)\n",
    "    gp.fit(x_train, y_train)\n",
    "\n",
    "    # Get mean and standard deviation for each possible\n",
    "    # number of hidden units\n",
    "    y_mean, y_var = gp.predict(x_test, eval_MSE=True)\n",
    "    y_std = np.sqrt(vector_2d(y_var))\n",
    "\n",
    "    return y_mean, y_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这个函数先计算每个可能隐层个数的EI，然后返回使得EI最大的隐层数量\n",
    "\n",
    "def next_parameter_by_ei(y_min, y_mean, y_std, x_choices):\n",
    "    # Calculate expecte improvement from 95% confidence interval\n",
    "    expected_improvement = y_min - (y_mean - 1.96 * y_std)\n",
    "    expected_improvement[expected_improvement < 0] = 0\n",
    "\n",
    "    max_index = expected_improvement.argmax()\n",
    "    # Select next choice\n",
    "    next_parameter = x_choices[max_index]\n",
    "\n",
    "    return next_parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通过若干轮 gaussian process估计--EI选择 来找最佳参数\n",
    "\n",
    "import random\n",
    "\n",
    "def hyperparam_selection(func, n_hidden_range, func_args=None, n_iter=20):\n",
    "    if func_args is None:\n",
    "        func_args = []\n",
    "\n",
    "    scores = []\n",
    "    parameters = []\n",
    "\n",
    "    min_n_hidden, max_n_hidden = n_hidden_range\n",
    "    n_hidden_choices = np.arange(min_n_hidden, max_n_hidden + 1)\n",
    "\n",
    "    # To be able to perform gaussian process we need to\n",
    "    # have at least 2 samples.\n",
    "    n_hidden = random.randint(min_n_hidden, max_n_hidden)\n",
    "    score = func(n_hidden, *func_args)\n",
    "\n",
    "    parameters.append(n_hidden)\n",
    "    scores.append(score)\n",
    "\n",
    "    n_hidden = random.randint(min_n_hidden, max_n_hidden)\n",
    "\n",
    "    for iteration in range(2, n_iter + 1):\n",
    "        \n",
    "        print(\"n_hidden:\", n_hidden)\n",
    "        \n",
    "        score = func(n_hidden, *func_args)\n",
    "\n",
    "        parameters.append(n_hidden)\n",
    "        scores.append(score)\n",
    "\n",
    "        y_min = min(scores)\n",
    "        y_mean, y_std = gaussian_process(parameters, scores,\n",
    "                                         n_hidden_choices)\n",
    "\n",
    "        n_hidden = next_parameter_by_ei(y_min, y_mean, y_std,\n",
    "                                        n_hidden_choices)\n",
    "\n",
    "        if y_min == 0 or n_hidden in parameters:\n",
    "            # Lowest expected improvement value have been achieved\n",
    "            break\n",
    "\n",
    "    min_score_index = np.argmin(scores)\n",
    "    return parameters[min_score_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 运行\n",
    "\n",
    "best_n_hidden = hyperparam_selection(\n",
    "    train_network,\n",
    "    n_hidden_range=[50, 1000],\n",
    "    func_args=[x_train, x_test, y_train, y_test],\n",
    "    n_iter=6,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GP with EI 的一些缺点："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. 不能很好地处理一些类别数据，比如激活函数的类型  \n",
    "> 2. GP本身需要设置一些超参数  \n",
    "> 3. 当超参数数量多时会很慢  \n",
    "> 4. NN运行中会有一些随机化操作（如dropout），估计的最优未必最优"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree-structured Parzen Estimators (TPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意！GP法是直接对p(y|x)进行建模，即给定x，建模y服从的分布；而TPE法则是对p(x|y)进行建模，即给定y的条件下，建模x服从的分布。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![p](./TPE-model.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. 开始的几次迭代，需要若干次random search来warm up TPE   \n",
    "> 2. 在收集一些数据后，就可以运用TPE。要将所有observations分成两部分，第一部分具有好效果，第二部分是其他observations  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![p6](./tpe-observation-groups.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "之后便不再需要具体的best observations，只要其分布。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "随后TPE将为两组建立各自的似然概率（GP构建的是各值的后验概率）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "于是Expected Inprovement的定义：$EI(x)=\\frac{l(x)}{g(x)}$  \n",
    "$l(x)$是x属于第一组的概率  \n",
    "$g(x)$是x属于第二组的概率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![p7](./tpe-sampled-candidates.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "l(x)与g(x)的估计方法：parzen-window density estimators  \n",
    "每个样本点定义了一个具有特定均值和标准差的高斯分布，将某组的所有点的分布堆叠起来，再归一化，便可以得到该类的pdf。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![p8](./parzen-estimators.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tree-structured是指超参数空间以树的形式组织。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GP法不能支持类别数据，因为GP法本质上要拟合出gaussian process的函数，这要求定义域连续；而TPE用密度估计的方法，就不需要连续，此时EI所选的点不会落在两个类别之间。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面使用MNIST数据集来举个TPE算法的例子："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用hyperopt库进行超参数的选择，它实现了TPE算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pprint import pprint\n",
    "from functools import partial\n",
    "\n",
    "import theano\n",
    "import numpy as np\n",
    "from sklearn import model_selection, datasets, preprocessing, metrics\n",
    "import hyperopt\n",
    "from hyperopt import hp\n",
    "from neupy import algorithms, layers, environment\n",
    "from neupy.exceptions import StopTraining\n",
    "\n",
    "theano.config.floatX = 'float32'\n",
    "\n",
    "def on_epoch_end(network):\n",
    "    if network.errors.last() > 10:\n",
    "        raise StopTraining(\"Training was interrupted. Error is to high.\")\n",
    "\n",
    "def train_network(parameters):\n",
    "    print(\"Parameters:\")\n",
    "    pprint(parameters)\n",
    "    print()\n",
    "\n",
    "    step = parameters['step']\n",
    "    batch_size = int(parameters['batch_size'])\n",
    "    proba = parameters['dropout']\n",
    "    activation_layer = parameters['act_func_type']\n",
    "    layer_sizes = [int(n) for n in parameters['layers']['n_units_layer']]\n",
    "    \n",
    "    network = layers.Input(784)\n",
    "\n",
    "    for layer_size in layer_sizes:\n",
    "        network = network > activation_layer(layer_size)\n",
    "\n",
    "    network = network > layers.Dropout(proba) > layers.Softmax(10)\n",
    "    \n",
    "    mnet = algorithms.RMSProp(\n",
    "        network,\n",
    "\n",
    "        batch_size=batch_size,\n",
    "        step=step,\n",
    "        \n",
    "        error='categorical_crossentropy',\n",
    "        shuffle_data=True,\n",
    "        \n",
    "        epoch_end_signal=on_epoch_end,\n",
    "    )\n",
    "    mnet.train(x_train, y_train, epochs=50)\n",
    "    \n",
    "    score = mnet.prediction_error(x_test, y_test)\n",
    "    \n",
    "    y_predicted = mnet.predict(x_test).argmax(axis=1)\n",
    "    accuracy = metrics.accuracy_score(y_test.argmax(axis=1), y_predicted)\n",
    "    \n",
    "    print(\"Final score: {}\".format(score))\n",
    "    print(\"Accuracy: {:.2%}\".format(accuracy))\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造超参数的先验分布\n",
    "\n",
    "def uniform_int(name, lower, upper):\n",
    "    # `quniform` returns:\n",
    "    # round(uniform(low, high) / q) * q\n",
    "    return hp.quniform(name, lower, upper, q=1)\n",
    "\n",
    "def loguniform_int(name, lower, upper):\n",
    "    # Do not forget to make a logarithm for the\n",
    "    # lower and upper bounds.\n",
    "    return hp.qloguniform(name, np.log(lower), np.log(upper), q=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist_dataset():\n",
    "    mnist = datasets.fetch_mldata('MNIST original')\n",
    "\n",
    "    target_scaler = preprocessing.OneHotEncoder()\n",
    "    target = mnist.target.reshape((-1, 1))\n",
    "    target = target_scaler.fit_transform(target).todense()\n",
    "\n",
    "    data = mnist.data / 255.\n",
    "    data = data - data.mean(axis=0)\n",
    "\n",
    "    x_train, x_test, y_train, y_test = model_selection.train_test_split(\n",
    "        data.astype(np.float32),\n",
    "        target.astype(np.float32),\n",
    "        train_size=(6 / 7.)\n",
    "    )\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "environment.reproducible()\n",
    "x_train, x_test, y_train, y_test = load_mnist_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造超参数的先验分布\n",
    "\n",
    "# Object stores all information about each trial.\n",
    "# Also, it stores information about the best trial.\n",
    "trials = hyperopt.Trials()\n",
    "\n",
    "parameter_space = {\n",
    "    'step': hp.uniform('step', 0.01, 0.5),\n",
    "    'layers': hp.choice('layers', [{\n",
    "        'n_layers': 1,\n",
    "        'n_units_layer': [\n",
    "            uniform_int('n_units_layer_11', 50, 500),\n",
    "        ],\n",
    "    }, {\n",
    "        'n_layers': 2,\n",
    "        'n_units_layer': [\n",
    "            uniform_int('n_units_layer_21', 50, 500),\n",
    "            uniform_int('n_units_layer_22', 50, 500),\n",
    "        ],\n",
    "    }]),\n",
    "    'act_func_type': hp.choice('act_func_type', [\n",
    "        layers.Relu,\n",
    "        layers.PRelu,\n",
    "        layers.Elu,\n",
    "        layers.Tanh,\n",
    "        layers.Sigmoid\n",
    "    ]),\n",
    "    \n",
    "    'dropout': hp.uniform('dropout', 0, 0.5),\n",
    "    'batch_size': loguniform_int('batch_size', 16, 512),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpe = partial(\n",
    "    hyperopt.tpe.suggest,\n",
    "\n",
    "    # Sample 1000 candidate and select candidate that\n",
    "    # has highest Expected Improvement (EI)\n",
    "    n_EI_candidates=1000,\n",
    "    \n",
    "    # Use 20% of best observations to estimate next\n",
    "    # set of parameters\n",
    "    gamma=0.2,\n",
    "    \n",
    "    # First 20 trials are going to be random\n",
    "    n_startup_jobs=20,\n",
    ")\n",
    "\n",
    "hyperopt.fmin(\n",
    "    train_network,\n",
    "    trials=trials,\n",
    "    space=parameter_space,\n",
    "\n",
    "    # Set up TPE for hyperparameter optimization\n",
    "    algo=tpe,\n",
    "\n",
    "    # Maximum number of iterations. Basically it trains at\n",
    "    # most 200 networks before choose the best one.\n",
    "    max_evals=200,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
