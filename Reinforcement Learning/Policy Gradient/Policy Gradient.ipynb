{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 问题描述"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用OpenAI Gym中的CartPole，如下图所示："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SegmentLocal](./CartPole.gif \"segment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本问题的状态包含四个元素：滑块的位置，滑块的速度，杆子的角度，杆子顶端的速度。动作包括滑块向左或向右移动。当杆子保持平衡则获得值为1的奖励，当杆子倒下则此episode结束。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(2)\n",
      "Box(4,)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "print(env.action_space)\n",
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent的策略用一个神经网络来表示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.01\n",
    "gamma = 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "policy gradient：$\\nabla_{\\theta}J(\\theta)=\\frac{1}{N}\\sum_{i=1}^N[(\\sum_{t=1}^T \\nabla_{\\theta}\\log\\pi_{\\theta}(a_{i,t}|s_{i,t}))(\\sum_{t=1}^T r(s_{i,t}|a_{i,t}))]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        self.state_space = env.observation_space.shape[0]\n",
    "        self.action_space = env.action_space.n\n",
    "        \n",
    "        self.l1 = nn.Linear(self.state_space, 128, bias=False)\n",
    "        self.l2 = nn.Linear(128, self.action_space, bias=False)\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # Episode policy and reward history \n",
    "        # 用来计算policy gradient\n",
    "        self.log_probs = []\n",
    "        self.reward_episode = []\n",
    "\n",
    "    def forward(self, x):    \n",
    "        model = torch.nn.Sequential(\n",
    "            self.l1,\n",
    "            nn.Dropout(p=0.6),\n",
    "            nn.ReLU(),\n",
    "            self.l2,\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        return model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = Policy()\n",
    "optimizer = optim.Adam(policy.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "policy将为每个动作（向左或向右）返回一个概率，下面这个函数的输入为当前状态，将此状态输入policy得到输出action，再将其转化成一个动作返回。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import Categorical\n",
    "\n",
    "# Select an action (0 or 1) by running policy model and choosing based on the probabilities in state\n",
    "def select_action(state):\n",
    "    #Select an action (0 or 1) by running policy model and choosing based on the probabilities in state\n",
    "    state = torch.from_numpy(state).type(torch.FloatTensor)\n",
    "    probs = policy(state)\n",
    "    m = Categorical(probs)         # 此处m是类别分布，假如probs=[0.3,0.7]，则对m采样可能获得0或1\n",
    "    action = m.sample()            # P(action=0)=0.3 P(action=1)=0.7\n",
    "    \n",
    "    policy.log_probs.append(m.log_prob(action))    # 注意！！这里应该是对数似然！！\n",
    "                                                   # action.item()返回0或1，m.log_prob(action)返回action所对应概率的对数\n",
    "    return action.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy Gradient每次对policy的参数$\\theta$做更新，更新公式如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Delta\\theta = \\alpha\\nabla_{\\theta}\\log \\pi_{\\theta}(s_t,a_t)v_t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\log \\pi_{\\theta}(s_t,a_t)$在select_action函数中已经记在了policy_histroy内。这里主要关心$v_t$，其表达式如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$v_t=\\sum_{k=0}^N \\gamma^{k} r_t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由此可见在第t步的reward为此步的reward和之后的reward的加权和。这样如果此episode越长，那么这一步得到的reward就越大。下面的代码用rewards来记录每一步的reward，rewards与policy_histroy的内积即为该网络的loss。由此可以得到policy gradient的一种直观理解——如果某一步的reward大，那么其action发生的概率应该尽可能大；反之则action发生概率应该尽可能小，这正是优化的目标！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss为-log(prob)$*$reward，希望loss尽可能小就是希望log(prob)$*$reward尽可能大，即当reward为正时希望$\\theta$使log(prob)尽量大；当reward为负时希望$\\theta$使log(prob)尽量小！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def update_policy():\n",
    "    R = 0\n",
    "    rewards = []\n",
    "    \n",
    "    # Discount future rewards back to the present using gamma\n",
    "    for r in policy.reward_episode[::-1]:\n",
    "        R = r + policy.gamma * R\n",
    "        rewards.insert(0,R)\n",
    "        \n",
    "    # Scale rewards\n",
    "    rewards = torch.FloatTensor(rewards)\n",
    "    rewards = (rewards - rewards.mean()) / (rewards.std())\n",
    "    \n",
    "    # Calculate loss\n",
    "    loss = []\n",
    "    for log_prob, reward in zip(policy.log_probs, rewards):\n",
    "        loss.append(-log_prob * reward)\n",
    "        \n",
    "    policy_loss = torch.stack(loss).sum()\n",
    "    \n",
    "    # Update network weights\n",
    "    optimizer.zero_grad()\n",
    "    policy_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    del policy.reward_episode[:]\n",
    "    del policy.log_probs[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面的代码是训练网络的主循环，一共进行episodes轮，每轮1000步（杆子倒下则停止），每步记录下reward和policy；在此episode结束后更新policy。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main(episodes):\n",
    "    running_reward = 10\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset() # Reset environment and record the starting state\n",
    "        done = False       \n",
    "    \n",
    "        for time in range(1000):\n",
    "            action = select_action(state)\n",
    "            # Step through environment using chosen action\n",
    "            state, reward, done, _ = env.step(action)\n",
    "\n",
    "            # Save reward\n",
    "            policy.reward_episode.append(reward)\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Used to determine when the environment is solved.\n",
    "        running_reward = (running_reward * 0.99) + (time * 0.01)\n",
    "\n",
    "        update_policy()\n",
    "\n",
    "        if episode % 50 == 0:\n",
    "            print('Episode {}\\tLast length: {:5d}\\tAverage length: {:.2f}'.format(episode, time, running_reward))\n",
    "\n",
    "        if running_reward > env.spec.reward_threshold:\n",
    "            print(\"Solved! Running reward is now {} and the last episode runs to {} time steps!\".format(running_reward, time))\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "开始运行模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0\tLast length:    27\tAverage length: 10.17\n",
      "Episode 50\tLast length:    38\tAverage length: 17.82\n",
      "Episode 100\tLast length:   100\tAverage length: 36.72\n",
      "Episode 150\tLast length:   248\tAverage length: 65.19\n",
      "Episode 200\tLast length:   499\tAverage length: 127.43\n",
      "Episode 250\tLast length:   147\tAverage length: 222.56\n",
      "Episode 300\tLast length:   499\tAverage length: 275.76\n",
      "Episode 350\tLast length:   499\tAverage length: 362.18\n",
      "Episode 400\tLast length:   499\tAverage length: 411.22\n",
      "Episode 450\tLast length:   499\tAverage length: 443.99\n",
      "Episode 500\tLast length:   499\tAverage length: 453.89\n",
      "Episode 550\tLast length:   499\tAverage length: 470.46\n",
      "Solved! Running reward is now 475.183308304331 and the last episode runs to 499 time steps!\n"
     ]
    }
   ],
   "source": [
    "episodes = 1000\n",
    "main(episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
